{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Example NPZ FILE\n!mkdir -p data/PittsburghBridge\n!wget -P data/PittsburghBridge https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:43:40.947310Z","iopub.execute_input":"2025-01-09T20:43:40.947682Z","iopub.status.idle":"2025-01-09T20:43:42.132317Z","shell.execute_reply.started":"2025-01-09T20:43:40.947649Z","shell.execute_reply":"2025-01-09T20:43:42.131075Z"}},"outputs":[{"name":"stdout","text":"--2025-01-09 20:43:41--  https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.164.78.121, 18.164.78.81, 18.164.78.128, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.164.78.121|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5701352 (5.4M) [application/zip]\nSaving to: ‘data/PittsburghBridge/pointcloud.npz’\n\npointcloud.npz      100%[===================>]   5.44M  12.1MB/s    in 0.4s    \n\n2025-01-09 20:43:42 (12.1 MB/s) - ‘data/PittsburghBridge/pointcloud.npz’ saved [5701352/5701352]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nimport torch\n\nneed_pytorch3d = False\ntry:\n    import pytorch3d\nexcept ModuleNotFoundError:\n    need_pytorch3d = True\nif need_pytorch3d:\n    pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n    version_str = \"\".join([\n        f\"py3{sys.version_info.minor}_cu\",\n        torch.version.cuda.replace(\".\", \"\"),\n        f\"_pyt{pyt_version_str}\"\n    ])\n    !pip install iopath\n    if sys.platform.startswith(\"linux\"):\n        print(\"Trying to install wheel for PyTorch3D\")\n        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n        pip_list = !pip freeze\n        need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for i in pip_list)\n    if need_pytorch3d:\n        print(f\"failed to find/install wheel for {version_str}\")\nif need_pytorch3d:\n    print(\"Installing PyTorch3D from source\")\n    !pip install ninja\n    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'","metadata":{"ExecuteTime":{"end_time":"2024-12-26T14:58:53.585274Z","start_time":"2024-12-26T14:58:48.836644Z"},"execution":{"iopub.status.busy":"2025-01-09T20:43:42.133936Z","iopub.execute_input":"2025-01-09T20:43:42.134364Z","iopub.status.idle":"2025-01-09T20:44:00.613171Z","shell.execute_reply.started":"2025-01-09T20:43:42.134331Z","shell.execute_reply":"2025-01-09T20:44:00.611870Z"},"id":"sRNWfRMnIzuJ","outputId":"cbebc8f4-6572-4a67-a33e-060c1d89ce4e","trusted":true},"outputs":[{"name":"stdout","text":"Collecting iopath\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath) (4.66.5)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.12.2)\nCollecting portalocker (from iopath)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: iopath\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=60009e8417d5188a75d106de3ca3f5e4e10b673af3dcd44a3ac6e3f2fe706c71\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built iopath\nInstalling collected packages: portalocker, iopath\nSuccessfully installed iopath-0.1.10 portalocker-3.1.1\nTrying to install wheel for PyTorch3D\nLooking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt241/download.html\nCollecting pytorch3d\n  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt241/pytorch3d-0.7.8-cp310-cp310-linux_x86_64.whl (20.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.10)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.66.5)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.12.2)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (3.1.1)\nInstalling collected packages: pytorch3d\nSuccessfully installed pytorch3d-0.7.8\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pytorch3d.structures import Pointclouds\n\ndef bounding_sphere_normalize(points: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    points: (N,3) tensor of point coords\n    Return normalized points in a unit sphere centered at origin.\n    \"\"\"\n    center = points.mean(dim=0, keepdim=True)\n    max_dist = (points - center).norm(p=2, dim=1).max()\n    points_normed = (points - center) / max_dist\n    return points_normed\n\n\ndef load_3d_data(file_path, num_points=10000, device=\"cuda\", do_normalize=True):\n    # Load NPZ point cloud directly like in the example\n    pointcloud = np.load(file_path)\n    verts = torch.Tensor(pointcloud['verts']).to(device)\n    rgb = torch.Tensor(pointcloud['rgb']).to(device)\n\n    # Subsample if needed\n    if len(verts) > num_points:\n        idx = torch.randperm(len(verts))[:num_points]\n        verts = verts[idx]\n        rgb = rgb[idx]\n\n    if do_normalize:\n        verts = bounding_sphere_normalize(verts)\n\n    # Return both the points tensor and the Pointclouds object\n    point_cloud = Pointclouds(points=[verts], features=[rgb])\n    return point_cloud  # Return both\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-09T20:44:00.615193Z","iopub.execute_input":"2025-01-09T20:44:00.615869Z","iopub.status.idle":"2025-01-09T20:44:00.637102Z","shell.execute_reply.started":"2025-01-09T20:44:00.615843Z","shell.execute_reply":"2025-01-09T20:44:00.636195Z"},"id":"fxo1SjSH2NHm","outputId":"fd9c90bb-1f98-4e61-ee27-b43b8f1bbd64","trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from itertools import islice\n\nimport torch\nfrom pytorch3d.structures import Pointclouds\nfrom pytorch3d.renderer import (\n    look_at_view_transform,\n    FoVOrthographicCameras,\n    PointsRasterizationSettings,\n    PointsRenderer,\n    PointsRasterizer,\n    AlphaCompositor\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport torchvision.transforms as T\n\n\nclass MultiViewPointCloudRenderer:\n    def __init__(self, image_size=512, base_dist=20, base_elev=10, base_azim=0,\n                 device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n        self.device = device\n        self.image_size = image_size\n        self.base_dist = base_dist\n        self.base_elev = base_elev\n        self.base_azim = base_azim\n        self.to_tensor = T.Compose([\n            T.Resize((image_size, image_size)),\n            T.ToTensor()\n        ])\n\n        # Define the settings for rasterization\n        self.raster_settings = PointsRasterizationSettings(\n            image_size=image_size,\n            radius=0.003,\n            points_per_pixel=10\n        )\n\n        # Define all views relative to base view\n        self.views = {\n            'Default': (base_dist, base_elev, base_azim),\n            'Y_90deg': (base_dist, base_elev, base_azim + 90),\n            'Y_180deg': (base_dist, base_elev, base_azim + 180),\n            'Y_-90deg': (base_dist, base_elev, base_azim - 90),\n            'X_90deg': (base_dist, base_elev + 90, base_azim),\n            'X_-90deg': (base_dist, base_elev - 90, base_azim),\n        }\n\n\n    def get_center_point(self, point_cloud):\n        \"\"\"Calculate the center point of the point cloud\"\"\"\n        points = point_cloud.points_packed()\n        center = torch.mean(points, dim=0)\n        return center.unsqueeze(0)  # Add batch dimension\n\n    def create_renderer(self, dist, elev, azim, center_point, background_color=(0, 0, 0)):\n        \"\"\"Create a renderer for specific camera parameters\"\"\"\n        # Use the center point as the 'at' parameter\n        R, T = look_at_view_transform(\n            dist=dist,\n            elev=elev,\n            azim=azim,\n            at=center_point,  # Look at the center of the point cloud\n        )\n        cameras = FoVOrthographicCameras(device=self.device, R=R, T=T, znear=0.01)\n\n        rasterizer = PointsRasterizer(cameras=cameras, raster_settings=self.raster_settings)\n        renderer = PointsRenderer(\n            rasterizer=rasterizer,\n            compositor=AlphaCompositor(background_color=background_color)\n        )\n        return renderer\n\n    def load_background(self, background_path):\n        bg_image = Image.open(background_path)\n        bg_tensor = self.to_tensor(bg_image).to(self.device)\n        return bg_tensor.permute(1, 2, 0)  # Convert to HWC format\n\n    def render_all_views(self, point_cloud, n_views=6, background_path=None,background_color=(0, 0, 0)):\n        images = {}\n        center_point = self.get_center_point(point_cloud)\n\n        if background_path:\n            background = self.load_background(background_path)\n        else:\n            background = None\n\n        for view_name, (dist, elev, azim) in islice(self.views.items(), n_views):\n            renderer = self.create_renderer(dist, elev, azim, center_point,background_color=background_color)\n            image = renderer(point_cloud)\n\n            if background is not None:\n                # Create binary mask from points\n                mask = torch.any(image[0, ..., :3] > 0, dim=-1).float()\n                mask = mask.unsqueeze(-1).expand(-1, -1, 3)\n                composite = (image[0, ..., :3] * mask) + (background * (1 - mask))\n                images[view_name] = composite\n            else:\n                images[view_name] = image[0, ..., :3]\n\n        return images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:44:00.638413Z","iopub.execute_input":"2025-01-09T20:44:00.638806Z","iopub.status.idle":"2025-01-09T20:44:02.170911Z","shell.execute_reply.started":"2025-01-09T20:44:00.638770Z","shell.execute_reply":"2025-01-09T20:44:02.170067Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torchvision\n\ndef save_results(point_cloud, renderer,n_views,device,output_dir,output_name):\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    rendered_images = renderer.render_all_views(point_cloud=point_cloud, n_views=n_views,background_color = (1,1,1))\n    # Convert dictionary of images to tensor\n    rendered_tensor = []\n    for name, img in rendered_images.items():\n        rendered_tensor.append(img.to(device))\n    rendered_tensor = torch.stack(rendered_tensor)\n\n    # Convert rendered images to CLIP format\n    rendered_images = rendered_tensor.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n\n    # Convert to uint8 range [0, 255]\n    rendered_images = (rendered_images * 255).clamp(0, 255).to(torch.uint8)\n\n    # Save rendered image using torchvision\n    torchvision.utils.save_image(\n        rendered_images.float() / 255.0,  # Convert back to [0,1] range\n        os.path.join(output_dir, output_name),\n         normalize=False  # We've already normalized the values\n     )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:44:02.171810Z","iopub.execute_input":"2025-01-09T20:44:02.172193Z","iopub.status.idle":"2025-01-09T20:44:02.178795Z","shell.execute_reply.started":"2025-01-09T20:44:02.172167Z","shell.execute_reply":"2025-01-09T20:44:02.177626Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device=\"cuda\"\n\npoint_cloud = load_3d_data(\n    \"/kaggle/working/data/PittsburghBridge/pointcloud.npz\",\n    num_points=100000\n)\n\n\nrenderer = MultiViewPointCloudRenderer(\n    image_size=512,\n    base_dist=30,  # Your default view distance\n    base_elev=10,  # Your default elevation\n    base_azim=45,  # Your default azimuth\n    device=device\n)\n\nsave_results(\n    point_cloud=point_cloud,\n    renderer=renderer,\n    n_views=1,\n    output_dir=\"./output\",\n    output_name=\"point_cloud.png\",\n    device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:44:02.179708Z","iopub.execute_input":"2025-01-09T20:44:02.180022Z","iopub.status.idle":"2025-01-09T20:44:03.497189Z","shell.execute_reply.started":"2025-01-09T20:44:02.179983Z","shell.execute_reply":"2025-01-09T20:44:03.496180Z"}},"outputs":[],"execution_count":6}]}