{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.0_cu111.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from itertools import permutations, product\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import device, color_mesh\n",
    "\n",
    "\n",
    "def get_clip_model(clipmodel=\"ViT-L/14\"):\n",
    "    \"\"\"\n",
    "    Loads and configures a CLIP model for text-guided 3D highlighting.\n",
    "\n",
    "    Args:\n",
    "        clipmodel (str): Name of the CLIP model to use. Default is \"ViT-L/14\".\n",
    "                        Other options include \"ViT-L/14@336px\", \"RN50x4\", \"RN50x16\", \"RN50x64\"\n",
    "\n",
    "    Returns:\n",
    "        tuple: (clip_model, preprocess, resolution)\n",
    "            - clip_model: The loaded CLIP model\n",
    "            - preprocess: CLIP's preprocessing transform\n",
    "            - resolution: The appropriate resolution for the model\n",
    "    \"\"\"\n",
    "    import clip\n",
    "    from utils import device\n",
    "\n",
    "    # Load the CLIP model and move to appropriate device\n",
    "    clip_model, preprocess = clip.load(clipmodel, device=device, jit=False)\n",
    "\n",
    "    # Determine the appropriate resolution for the model\n",
    "    resolution = 224  # Default resolution\n",
    "    if clipmodel == \"ViT-L/14@336px\":\n",
    "        resolution = 336\n",
    "    elif clipmodel == \"RN50x4\":\n",
    "        resolution = 288\n",
    "    elif clipmodel == \"RN50x16\":\n",
    "        resolution = 384\n",
    "    elif clipmodel == \"RN50x64\":\n",
    "        resolution = 448\n",
    "\n",
    "    # Freeze the model parameters\n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    clip_model.eval()\n",
    "\n",
    "    return clip_model, preprocess, resolution\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "        \n",
    "\n",
    "def clip_loss(rendered_images, encoded_text, clip_transform, augment_transform, clip_model, n_augs=5, clipavg=\"view\"):\n",
    "    \"\"\"\n",
    "    Calculates the CLIP-based loss between rendered images and text description.\n",
    "\n",
    "    The loss measures how well the highlighted regions match the text description\n",
    "    by comparing their CLIP embeddings. Lower loss means better alignment.\n",
    "\n",
    "    Args:\n",
    "        rendered_images: Tensor of rendered views of the mesh\n",
    "        encoded_text: CLIP embedding of the target text description\n",
    "        clip_transform: Basic CLIP preprocessing transform\n",
    "        augment_transform: Transform for data augmentation\n",
    "        clip_model: The CLIP model for computing embeddings\n",
    "        n_augs: Number of augmentations to apply (default: 5)\n",
    "        clipavg: Method for averaging CLIP scores (\"view\" or \"embedding\")\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed loss value\n",
    "    \"\"\"\n",
    "    # If no augmentations requested, just use basic transform\n",
    "    if n_augs == 0:\n",
    "        # Apply CLIP's preprocessing transform\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "\n",
    "        # Get image embeddings from CLIP\n",
    "        encoded_renders = clip_model.encode_image(clip_image)\n",
    "\n",
    "        # Normalize embeddings to lie on unit sphere\n",
    "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # Average across views or compare each view individually\n",
    "        if clipavg == \"view\":\n",
    "            # Handle both single and multiple text embeddings\n",
    "            if encoded_text.shape[0] > 1:\n",
    "                # Multiple text embeddings: average both image and text embeddings\n",
    "                loss = torch.cosine_similarity(\n",
    "                    torch.mean(encoded_renders, dim=0),\n",
    "                    torch.mean(encoded_text, dim=0),\n",
    "                    dim=0\n",
    "                )\n",
    "            else:\n",
    "                # Single text embedding: just average image embeddings\n",
    "                loss = torch.cosine_similarity(\n",
    "                    torch.mean(encoded_renders, dim=0, keepdim=True),\n",
    "                    encoded_text\n",
    "                )\n",
    "        else:\n",
    "            # Compare each view individually and average the similarities\n",
    "            loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
    "\n",
    "    # If augmentations requested, apply them and average results\n",
    "    else:\n",
    "        loss = 0.0\n",
    "        # Run multiple augmentations and average their losses\n",
    "        for _ in range(n_augs):\n",
    "            # Apply random augmentation transforms\n",
    "            augmented_image = augment_transform(rendered_images)\n",
    "\n",
    "            # Get embeddings for augmented images\n",
    "            encoded_renders = clip_model.encode_image(augmented_image)\n",
    "            encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
    "\n",
    "            # Calculate loss based on averaging method\n",
    "            if clipavg == \"view\":\n",
    "                if encoded_text.shape[0] > 1:\n",
    "                    loss -= torch.cosine_similarity(\n",
    "                        torch.mean(encoded_renders, dim=0),\n",
    "                        torch.mean(encoded_text, dim=0),\n",
    "                        dim=0\n",
    "                    )\n",
    "                else:\n",
    "                    loss -= torch.cosine_similarity(\n",
    "                        torch.mean(encoded_renders, dim=0, keepdim=True),\n",
    "                        encoded_text\n",
    "                    )\n",
    "            else:\n",
    "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
    "\n",
    "    return loss\n",
    "    \n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def setup_clip_transforms(resolution=224):\n",
    "    \"\"\"\n",
    "    Creates the transformation pipelines needed for CLIP processing.\n",
    "\n",
    "    Args:\n",
    "        resolution (int): The target resolution for the images (depends on CLIP model)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (clip_transform, augment_transform)\n",
    "    \"\"\"\n",
    "    # CLIP's normalization values\n",
    "    clip_mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "    clip_std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "    # Basic CLIP transform - just resize and normalize\n",
    "    clip_transform = transforms.Compose([\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.Normalize(clip_mean, clip_std)\n",
    "    ])\n",
    "\n",
    "    # Augmentation transform - adds random perturbations\n",
    "    augment_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(resolution, scale=(1, 1)),\n",
    "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "        transforms.Normalize(clip_mean, clip_std)\n",
    "    ])\n",
    "\n",
    "    return clip_transform, augment_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_highlighter import NeuralHighlighter\n",
    "\n",
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "# Set a consistent seed for reproducibility\n",
    "seed = 0  # You can use any integer value\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "render_res = 224\n",
    "learning_rate = 0.0001\n",
    "n_iter = 2500\n",
    "res = 224\n",
    "obj_path = 'data/horse.obj'\n",
    "n_augs = 5\n",
    "output_dir = './output/'\n",
    "clip_model_name = 'ViT-L/14'\n",
    "\n",
    "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "\n",
    "\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "# Initialize variables\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "\n",
    "log_dir = output_dir\n",
    "\n",
    "\n",
    "# MLP Settings\n",
    "mlp =  NeuralHighlighter(\n",
    "    depth=5,           # Number of hidden layers\n",
    "    width=256,         # Width of each layer\n",
    "    out_dim=2,         # Binary classification (highlight/no-highlight)\n",
    "    input_dim=3,       # 3D coordinates (x,y,z)\n",
    "    positional_encoding=False  # As recommended in the paper\n",
    ").to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "# list of possible colors\n",
    "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
    "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
    "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
    "colors = torch.tensor(full_colors).to(device)\n",
    "\n",
    "\n",
    "# --- Prompt ---\n",
    "# encode prompt with CLIP\n",
    "clip_model, preprocess, resolution = get_clip_model(clipmodel=clip_model_name)\n",
    "# Create your text prompt\n",
    "object_name = \"horse\"\n",
    "highlight_region = \"shoes\"\n",
    "prompt = \"A 3D render of a gray {} with highlighted {}\".format(object_name, highlight_region)\n",
    "\n",
    "# Encode the text using CLIP\n",
    "with torch.no_grad():\n",
    "    text_tokens = clip.tokenize([prompt]).to(device)\n",
    "    text_features = clip_model.encode_text(text_tokens)\n",
    "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "render = Renderer(dim=(resolution, resolution))\n",
    "vertices = copy.deepcopy(mesh.vertices)\n",
    "n_views = 5\n",
    "\n",
    "# Set up the transforms\n",
    "clip_transform, augment_transform = setup_clip_transforms(resolution)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Optimization loop\n",
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # predict highlight probabilities\n",
    "    pred_class = mlp(vertices)\n",
    "\n",
    "    # color and render mesh\n",
    "    sampled_mesh = mesh\n",
    "    color_mesh(pred_class, sampled_mesh, colors)\n",
    "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
    "                                                            show=False,\n",
    "                                                            center_azim=0,\n",
    "                                                            center_elev=0,\n",
    "                                                            std=1,\n",
    "                                                            return_views=True,\n",
    "                                                            lighting=True,\n",
    "                                                            background=background)\n",
    "\n",
    "    # Calculate CLIP Loss\n",
    "    loss_func = loss = clip_loss(rendered_images,\n",
    "                text_features,  # This was called encoded_text in the function\n",
    "                clip_transform,\n",
    "                augment_transform,\n",
    "                clip_model,\n",
    "                n_augs=5)\n",
    "    loss_func.backward(retain_graph=True)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    # update variables + record loss\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss_func.item())\n",
    "\n",
    "    # report results\n",
    "    if i % 100 == 0:\n",
    "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
    "        save_renders(log_dir, i, rendered_images)\n",
    "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
    "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
    "\n",
    "\n",
    "# save results\n",
    "save_final_results(log_dir, mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "# Save prompts\n",
    "with open(os.path.join(dir, prompt), \"w\") as f:\n",
    "    f.write('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
