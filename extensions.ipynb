{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "#git clone with your token\n!git clone --branch extensions https://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@github.com/amiralichangizi/Affordance3DHighlighter.git",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-10T15:01:47.553917Z",
     "iopub.execute_input": "2025-01-10T15:01:47.554263Z",
     "iopub.status.idle": "2025-01-10T15:01:49.698758Z",
     "shell.execute_reply.started": "2025-01-10T15:01:47.554236Z",
     "shell.execute_reply": "2025-01-10T15:01:49.697522Z"
    },
    "id": "2rqJobuCpQLi",
    "outputId": "67a93806-440e-4b7c-a56e-c43debd5ced5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'Affordance3DHighlighter'...\nremote: Enumerating objects: 360, done.\u001B[K\nremote: Counting objects: 100% (122/122), done.\u001B[K\nremote: Compressing objects: 100% (93/93), done.\u001B[K\nremote: Total 360 (delta 69), reused 67 (delta 29), pack-reused 238 (from 1)\u001B[K\nReceiving objects: 100% (360/360), 5.24 MiB | 13.30 MiB/s, done.\nResolving deltas: 100% (209/209), done.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "!git pull origin extensions",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-10T15:22:52.802401Z",
     "iopub.execute_input": "2025-01-10T15:22:52.802785Z",
     "iopub.status.idle": "2025-01-10T15:22:54.548463Z",
     "shell.execute_reply.started": "2025-01-10T15:22:52.802752Z",
     "shell.execute_reply": "2025-01-10T15:22:54.547610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "remote: Enumerating objects: 8, done.\u001B[K\nremote: Counting objects: 100% (8/8), done.\u001B[K\nremote: Compressing objects: 100% (4/4), done.\u001B[K\nremote: Total 6 (delta 4), reused 4 (delta 2), pack-reused 0 (from 0)\u001B[K\nUnpacking objects: 100% (6/6), 684 bytes | 97.00 KiB/s, done.\nFrom https://github.com/amiralichangizi/Affordance3DHighlighter\n * branch            extensions -> FETCH_HEAD\n   a4e0daa..105c0af  extensions -> origin/extensions\nUpdating a4e0daa..105c0af\nFast-forward\n extensions.ipynb | 7 \u001B[32m++++\u001B[m\u001B[31m---\u001B[m\n 1 file changed, 4 insertions(+), 3 deletions(-)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "import os\n\nos.chdir('/kaggle/working/Affordance3DHighlighter')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-10T15:01:56.367304Z",
     "iopub.execute_input": "2025-01-10T15:01:56.367597Z",
     "iopub.status.idle": "2025-01-10T15:01:56.371827Z",
     "shell.execute_reply.started": "2025-01-10T15:01:56.367574Z",
     "shell.execute_reply": "2025-01-10T15:01:56.370796Z"
    },
    "id": "fhLYzi952EEA",
    "trusted": true
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "!pip install gdown\n!gdown --id 1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\n!unzip full-shape.zip -d /kaggle/working/Affordance3DHighlighter/data/",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-10T15:01:59.534343Z",
     "iopub.execute_input": "2025-01-10T15:01:59.534634Z",
     "iopub.status.idle": "2025-01-10T15:02:37.395339Z",
     "shell.execute_reply.started": "2025-01-10T15:01:59.534612Z",
     "shell.execute_reply": "2025-01-10T15:02:37.394197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\nFrom (redirected): https://drive.google.com/uc?id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF&confirm=t&uuid=00531178-1f06-4546-a82b-e14c0dcf6b9d\nTo: /kaggle/working/Affordance3DHighlighter/full-shape.zip\n100%|████████████████████████████████████████| 558M/558M [00:07<00:00, 77.4MB/s]\nArchive:  full-shape.zip\n  inflating: /kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl  \n  inflating: /kaggle/working/Affordance3DHighlighter/data/full_shape_val_data.pkl  \n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load training data\n",
    "with open('/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl', 'rb') as train_file:\n",
    "    dataset = pickle.load(train_file)\n",
    "\n",
    "# Create a set to store unique semantic classes\n",
    "semantic_classes = set()\n",
    "all_affordances = dataset[0]['affordance']\n",
    "\n",
    "# Iterate through the dataset and collect semantic classes\n",
    "for item in dataset:\n",
    "    semantic_class = item['semantic class']\n",
    "    semantic_classes.add(semantic_class)\n",
    "\n",
    "# Print all unique semantic classes\n",
    "print(\"Unique semantic classes in the dataset:\")\n",
    "for cls in sorted(semantic_classes):\n",
    "    print(f\"- {cls}\")\n",
    "\n",
    "# Print all unique semantic classes\n",
    "print(\"Unique Affordances in the dataset:\")\n",
    "for cls in sorted(all_affordances):\n",
    "    print(f\"- {cls}\")\n",
    "\n",
    "# Print total count\n",
    "print(f\"\\nTotal number of unique semantic classes: {len(semantic_classes)}\")\n",
    "print(f\"\\nTotal number of unique Affordances: {len(all_affordances)}\")\n",
    "\n",
    "# Optional: Print count of items per semantic class\n",
    "class_counts = {}\n",
    "for item in dataset:\n",
    "    semantic_class = item['semantic class']\n",
    "    class_counts[semantic_class] = class_counts.get(semantic_class, 0) + 1\n",
    "\n",
    "print(\"\\nNumber of items per semantic class:\")\n",
    "for cls, count in sorted(class_counts.items()):\n",
    "    print(f\"- {cls}: {count} items\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-10T15:46:27.445283Z",
     "iopub.execute_input": "2025-01-10T15:46:27.445589Z",
     "iopub.status.idle": "2025-01-10T15:46:30.064214Z",
     "shell.execute_reply.started": "2025-01-10T15:46:27.445565Z",
     "shell.execute_reply": "2025-01-10T15:46:30.063483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Unique semantic classes in the dataset:\n- Bag\n- Bed\n- Bottle\n- Bowl\n- Chair\n- Clock\n- Dishwasher\n- Display\n- Door\n- Earphone\n- Faucet\n- Hat\n- Keyboard\n- Knife\n- Laptop\n- Microwave\n- Mug\n- Refrigerator\n- Scissors\n- StorageFurniture\n- Table\n- TrashCan\n- Vase\nUnique Affordances in the dataset:\n- contain\n- cut\n- displaY\n- grasp\n- layable\n- lift\n- listen\n- move\n- openable\n- pourable\n- press\n- pull\n- pushable\n- sittable\n- stab\n- support\n- wear\n- wrap_grasp\n\nTotal number of unique semantic classes: 23\n\nTotal number of unique Affordances: 18\n\nNumber of items per semantic class:\n- Bag: 88 items\n- Bed: 127 items\n- Bottle: 288 items\n- Bowl: 132 items\n- Chair: 4280 items\n- Clock: 368 items\n- Dishwasher: 117 items\n- Display: 622 items\n- Door: 154 items\n- Earphone: 157 items\n- Faucet: 441 items\n- Hat: 156 items\n- Keyboard: 110 items\n- Knife: 225 items\n- Laptop: 295 items\n- Microwave: 130 items\n- Mug: 133 items\n- Refrigerator: 130 items\n- Scissors: 49 items\n- StorageFurniture: 1531 items\n- Table: 5593 items\n- TrashCan: 221 items\n- Vase: 735 items\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": "!pip install git+https://github.com/openai/CLIP.git\n!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html",
   "metadata": {
    "id": "d8vCbctxbPP4",
    "outputId": "303a2d90-d6d8-4bbd-97fa-621867d8c52c",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\nimport sys\nimport torch\n\nneed_pytorch3d = False\ntry:\n    import pytorch3d\nexcept ModuleNotFoundError:\n    need_pytorch3d = True\nif need_pytorch3d:\n    pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n    version_str = \"\".join([\n        f\"py3{sys.version_info.minor}_cu\",\n        torch.version.cuda.replace(\".\", \"\"),\n        f\"_pyt{pyt_version_str}\"\n    ])\n    !pip install iopath\n    if sys.platform.startswith(\"linux\"):\n        print(\"Trying to install wheel for PyTorch3D\")\n        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n        pip_list = !pip freeze\n        need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for i in pip_list)\n    if need_pytorch3d:\n        print(f\"failed to find/install wheel for {version_str}\")\nif need_pytorch3d:\n    print(\"Installing PyTorch3D from source\")\n    !pip install ninja\n    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T14:58:53.585274Z",
     "start_time": "2024-12-26T14:58:48.836644Z"
    },
    "execution": {
     "iopub.status.busy": "2025-01-10T15:03:30.363989Z",
     "iopub.execute_input": "2025-01-10T15:03:30.364310Z",
     "iopub.status.idle": "2025-01-10T15:03:45.952957Z",
     "shell.execute_reply.started": "2025-01-10T15:03:30.364283Z",
     "shell.execute_reply": "2025-01-10T15:03:45.952071Z"
    },
    "id": "sRNWfRMnIzuJ",
    "outputId": "cbebc8f4-6572-4a67-a33e-060c1d89ce4e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting iopath\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.2/42.2 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath) (4.66.5)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.12.2)\nCollecting portalocker (from iopath)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: iopath\n  Building wheel for iopath (setup.py) ... \u001B[?25l\u001B[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=ca0106d38b6955a3c80384b95df08378cb39f9a88e509540ca93e1e15605b92a\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built iopath\nInstalling collected packages: portalocker, iopath\nSuccessfully installed iopath-0.1.10 portalocker-3.1.1\nTrying to install wheel for PyTorch3D\nLooking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt241/download.html\nCollecting pytorch3d\n  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt241/pytorch3d-0.7.8-cp310-cp310-linux_x86_64.whl (20.5 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.5/20.5 MB\u001B[0m \u001B[31m38.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.10)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.66.5)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.12.2)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (3.1.1)\nInstalling collected packages: pytorch3d\nSuccessfully installed pytorch3d-0.7.8\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "!pip install --ignore-installed open3d",
   "metadata": {
    "id": "zT9LfpcRXDHy",
    "outputId": "a506d6d8-8390-403f-ac0b-c079ebdf6aee",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from src.mesh import Mesh\n",
    "from pytorch3d.structures import Pointclouds\n",
    "\n",
    "from src.convertor import obj_to_pointcloud\n",
    "\n",
    "\n",
    "def bounding_sphere_normalize(points: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    points: (N,3) tensor of point coords\n",
    "    Return normalized points in a unit sphere centered at origin.\n",
    "    \"\"\"\n",
    "    center = points.mean(dim=0, keepdim=True)\n",
    "    max_dist = (points - center).norm(p=2, dim=1).max()\n",
    "    points_normed = (points - center) / max_dist\n",
    "    return points_normed\n",
    "\n",
    "\n",
    "def load_3d_data(file_path, num_points=10000, device=\"cuda\", do_normalize=True):\n",
    "    \"\"\"\n",
    "    Loads 3D data as PyTorch3D Pointclouds from either NPZ point cloud or OBJ mesh.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to either .npz point cloud or .obj mesh file\n",
    "        num_points: Number of points to sample if loading from mesh\n",
    "        device: Device to load data on\n",
    "\n",
    "    Returns:\n",
    "        Pointclouds object containing points and features\n",
    "    \"\"\"\n",
    "    file_ext = file_path.split('.')[-1].lower()\n",
    "\n",
    "    if file_ext == 'npz':\n",
    "        # Load NPZ point cloud directly like in the example\n",
    "        pointcloud = np.load(file_path)\n",
    "        verts = torch.Tensor(pointcloud['verts']).to(device)\n",
    "        rgb = torch.Tensor(pointcloud['rgb']).to(device)\n",
    "\n",
    "        print(\"lenght of the data\")\n",
    "        print(len(verts))\n",
    "\n",
    "        # Subsample if needed\n",
    "        if len(verts) > num_points:\n",
    "            idx = torch.randperm(len(verts))[:num_points]\n",
    "            verts = verts[idx]\n",
    "            rgb = rgb[idx]\n",
    "\n",
    "        if do_normalize:\n",
    "            verts = bounding_sphere_normalize(verts)\n",
    "\n",
    "        # Return both the points tensor and the Pointclouds object\n",
    "        point_cloud = Pointclouds(points=[verts], features=[rgb])\n",
    "        return verts, point_cloud  # Return both\n",
    "\n",
    "    elif file_ext == 'obj':\n",
    "        # Load and convert your OBJ file\n",
    "        points, point_cloud = obj_to_pointcloud(\n",
    "            file_path,\n",
    "            num_points=num_points,  # Adjust this number as needed\n",
    "            device=\"cuda\"  # Use \"cpu\" if you don't have a GPU\n",
    "        )\n",
    "        if do_normalize:\n",
    "            points = bounding_sphere_normalize(points)\n",
    "            # here we update the point cloud too\n",
    "            rgb = point_cloud.features_packed()  # shape [N,3]\n",
    "            point_cloud = Pointclouds(points=[points], features=[rgb])\n",
    "        return points, point_cloud\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_ext}. Only .npz and .obj are supported.\")\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-10T15:05:25.459408Z",
     "iopub.execute_input": "2025-01-10T15:05:25.459761Z",
     "iopub.status.idle": "2025-01-10T15:05:28.585774Z",
     "shell.execute_reply.started": "2025-01-10T15:05:25.459727Z",
     "shell.execute_reply": "2025-01-10T15:05:28.584885Z"
    },
    "id": "fxo1SjSH2NHm",
    "outputId": "fd9c90bb-1f98-4e61-ee27-b43b8f1bbd64",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Warp 1.5.1 initialized:\n   CUDA Toolkit 12.6, Driver 12.6\n   Devices:\n     \"cpu\"      : \"x86_64\"\n     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n     \"cuda:1\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n   CUDA peer access:\n     Supported fully (all-directional)\n   Kernel cache:\n     /root/.cache/warp/1.5.1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "from src.prompt_strategies import generate_affordance_prompt\n",
    "from src.evaluation_fullshapev2 import compute_mIoU\n",
    "from src.data_loader_fullshape import FullShapeDataset\n",
    "from src.render.cloud_point_renderer import MultiViewPointCloudRenderer\n",
    "from src.save_results import save_renders, save_results\n",
    "from src.neural_highlighter import NeuralHighlighter\n",
    "from src.Clip.loss_function import clip_loss\n",
    "from src.Clip.clip_model import get_clip_model, encode_text, setup_clip_transforms\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "# Set a consistent seed for reproducibility\n",
    "seed = 0  # You can use any integer value\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def optimize_point_cloud(points, clip_model, renderer, encoded_text, log_dir: str, **kwargs):\n",
    "    num_iterations = kwargs.get('num_iterations', 1000)\n",
    "    learning_rate = kwargs.get('learning_rate', 1e-4)\n",
    "    depth = kwargs.get('depth', 5)\n",
    "    width = kwargs.get('network_width', 256)\n",
    "    n_views = kwargs.get(\"n_views\", 4)\n",
    "    n_augs = kwargs.get('n_augs', 1)\n",
    "    clipavg = kwargs.get('clipavg', 'view')\n",
    "    device = kwargs.get('device', 'cuda')\n",
    "    background_path = kwargs.get(\"background_path\", None)\n",
    "\n",
    "    # Initialize network and optimizer\n",
    "    net = NeuralHighlighter(\n",
    "        depth=depth,  # Number of hidden layers\n",
    "        width=width,  # Width of each layer\n",
    "        out_dim=2,  # Binary classification (highlight/no-highlight)\n",
    "        input_dim=3,  # 3D coordinates (x,y,z)\n",
    "        positional_encoding=False  # As recommended in the paper\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Set up the transforms\n",
    "    clip_transform, augment_transform = setup_clip_transforms()\n",
    "\n",
    "    # Training loop\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict highlight probabilities\n",
    "        pred_class = net(points)\n",
    "\n",
    "        # Create colors based on predictions\n",
    "        highlight_color = torch.tensor([204 / 255, 1.0, 0.0]).to(device)\n",
    "        base_color = torch.tensor([180 / 255, 180 / 255, 180 / 255]).to(device)\n",
    "\n",
    "        colors = pred_class[:, 0:1] * highlight_color + pred_class[:, 1:2] * base_color\n",
    "\n",
    "        # Create and render point cloud\n",
    "        point_cloud = renderer.create_point_cloud(points, colors)\n",
    "        rendered_images = renderer.render_all_views(\n",
    "            point_cloud=point_cloud,\n",
    "            n_views=n_views,\n",
    "            background_path=background_path\n",
    "        )\n",
    "        # Convert dictionary of images to tensor\n",
    "        rendered_tensor = []\n",
    "        for name, img in rendered_images.items():\n",
    "            rendered_tensor.append(img.to(device))\n",
    "        rendered_tensor = torch.stack(rendered_tensor)\n",
    "\n",
    "        #Convert rendered images to CLIP format\n",
    "        rendered_images = rendered_tensor.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
    "        #print(rendered_images.shape)\n",
    "\n",
    "        # Calculate CLIP loss\n",
    "        loss = clip_loss(\n",
    "            rendered_images=rendered_images,\n",
    "            encoded_text=encoded_text,\n",
    "            clip_transform=clip_transform,\n",
    "            augment_transform=augment_transform,\n",
    "            clip_model=clip_model,\n",
    "            n_augs=n_augs,\n",
    "            clipavg=clipavg\n",
    "        )\n",
    "        #print(\"Loss computation graph:\")\n",
    "        #print_grad_fn(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n",
    "            save_renders(log_dir, i, rendered_images)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def main(input_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Main function for 3D highlighting with configurable parameters.\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to input 3D file (mesh or point cloud)\n",
    "        object_name: Name of the object for the prompt\n",
    "        highlight_region: Region to highlight\n",
    "        **kwargs: Optional parameters with defaults:\n",
    "            n_views: Number of views to render (default: 5)\n",
    "            n_aug: Number of augmentations (default: 5)\n",
    "            clipavg: Method for CLIP averaging (default: \"view\")\n",
    "            network_depth: Depth of neural network (default: 5)\n",
    "            network_width: Width of neural layers (default: 256)\n",
    "            learning_rate: Learning rate for optimization (default: 1e-4)\n",
    "            num_iterations: Number of training iterations (default: 500)\n",
    "            num_points: Number of points to sample (default: 10000)\n",
    "            device: Device to run on (default: \"cuda\")\n",
    "            output_dir: Directory for outputs (default: \"./output\")\n",
    "    \"\"\"\n",
    "    # Extract parameters from kwargs with defaults\n",
    "    n_views = kwargs.get(\"n_views\", 4)\n",
    "    num_points = kwargs.get(\"num_points\", 10000)\n",
    "    device = kwargs.get(\"device\", \"cuda\")\n",
    "    output_dir = kwargs.get(\"output_dir\", \"./output\")\n",
    "    do_normalize = kwargs.get(\"do_normalize\", True)\n",
    "    background_path = kwargs.get(\"background_path\", None)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # LOAD AffordanceNet Dataset\n",
    "        print(f\"Loading AffordanceNet Dataset...\")\n",
    "        if input_path.split(\".\")[-1] == \"pkl\":\n",
    "            dataset = FullShapeDataset(input_path, device=device)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid file format. Expected .pkl file, got: {input_path.split(\".\")[-1]}\")\n",
    "\n",
    "        val_indices = list(range(min(3, len(dataset))))\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Setup CLIP model\n",
    "        print(\"Setting up CLIP model...\")\n",
    "        clip_model, preprocess, resolution = get_clip_model()\n",
    "\n",
    "        # Initialize renderer\n",
    "        print(\"Setting up renderer...\")\n",
    "        renderer = MultiViewPointCloudRenderer(\n",
    "            camera_type=\"perspective\",\n",
    "            image_size=512,\n",
    "            base_dist=2.5,  # Your default view distance\n",
    "            base_elev=10,  # Your default elevation\n",
    "            base_azim=45,  # Your default azimuth\n",
    "            device=device,\n",
    "            point_radius=0.008\n",
    "        )\n",
    "\n",
    "        print(f\"Loading 3D data from {input_path}...\")\n",
    "\n",
    "        #Train on random Shape\n",
    "        global_mIou = 0\n",
    "        for idx in tqdm(enumerate(val_indices)):\n",
    "\n",
    "            shape_entry = dataset[idx]\n",
    "            shape_class = shape_entry[\"shape_class\"]\n",
    "            affordances = shape_entry[\"affordances\"]\n",
    "            label_dict = shape_entry[\"labels_dict\"]\n",
    "\n",
    "            # Convert coords to tensor if not already\n",
    "            points = shape_entry[\"coords\"]\n",
    "            if not isinstance(points, torch.Tensor):\n",
    "                points = torch.tensor(points, device=device)\n",
    "\n",
    "            print(f\"Loaded {len(points)} points\")\n",
    "            shape_subdir = os.path.join(output_dir, f\"shape_{shape_class}\")\n",
    "            os.makedirs(shape_subdir, exist_ok=True)\n",
    "\n",
    "            shape_mIOU = 0\n",
    "            for affordance in affordances:\n",
    "                \n",
    "                affordance_subdir = os.path.join(output_dir, f\"affordance_{affordance}\")\n",
    "                os.makedirs(affordance_subdir, exist_ok=True)\n",
    "                \n",
    "                # Create and encode prompt\n",
    "                prompt = generate_affordance_prompt(shape_class, affordance, strategy=\"affordance_specific\")\n",
    "                print(f\"Using prompt: {prompt}\")\n",
    "                text_features = encode_text(clip_model, prompt, device)\n",
    "\n",
    "                # Optimize point cloud highlighting\n",
    "                print(\"Starting optimization...\")\n",
    "                net = optimize_point_cloud(\n",
    "                    points=points,\n",
    "                    renderer=renderer,\n",
    "                    clip_model=clip_model,\n",
    "                    encoded_text=text_features,\n",
    "                    log_dir=affordance_subdir,\n",
    "                    **kwargs\n",
    "                )\n",
    "\n",
    "                #Compute IoU for *this* affordance\n",
    "                with torch.no_grad():\n",
    "                    pred_class = net(points)  # shape [N,2]\n",
    "                    highlight_scores = pred_class[:, 0]\n",
    "\n",
    "                    gt_bin = (label_dict[affordance] > 0.0).long()\n",
    "                    bin_pred = (highlight_scores >= 0.15).long()\n",
    "                    iou_val = compute_mIoU(bin_pred, gt_bin)\n",
    "                    print(f\"IoU: {iou_val}\")\n",
    "                    shape_mIOU += iou_val\n",
    "\n",
    "            shape_mIOU = shape_mIOU / len(affordances)\n",
    "            print(f\"shape mIOU: {shape_mIOU}\")\n",
    "            global_mIou += shape_mIOU\n",
    "        global_mIou = global_mIou / len(val_indices)\n",
    "        print(f\"global mIOU: {global_mIou}\")\n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-10T15:20:43.133975Z",
     "iopub.execute_input": "2025-01-10T15:20:43.134340Z",
     "iopub.status.idle": "2025-01-10T15:20:43.154153Z",
     "shell.execute_reply.started": "2025-01-10T15:20:43.134312Z",
     "shell.execute_reply": "2025-01-10T15:20:43.153263Z"
    },
    "id": "E0SBrmlBkwib",
    "trusted": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "sample_bg = \"/kaggle/working/Affordance3DHighlighter/data/background2.jpg\"\n",
    "main(\n",
    "    input_path=\"/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl\",\n",
    "    n_views=1,\n",
    "    n_augs=0,\n",
    "    clipavg=\"view\",\n",
    "    network_depth=4,\n",
    "    network_width=256,\n",
    "    learning_rate=1e-4,\n",
    "    num_iterations=1000,\n",
    "    num_points=100000,\n",
    "    device=\"cuda\",\n",
    "    output_dir=\"./output\",\n",
    "    background_path=None\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-10T15:40:14.969610Z",
     "iopub.execute_input": "2025-01-10T15:40:14.970021Z",
     "iopub.status.idle": "2025-01-10T15:41:31.395630Z",
     "shell.execute_reply.started": "2025-01-10T15:40:14.969994Z",
     "shell.execute_reply": "2025-01-10T15:41:31.394939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 1087 valid door objects with all affordances\nSetting up CLIP model...\nUsing prompt: A 3D render of a gray Vase emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nSetting up renderer...\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  0%|          | 2/1000 [00:00<01:17, 12.85it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.1873\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 10%|█         | 104/1000 [00:06<00:58, 15.35it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.1873\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 20%|██        | 202/1000 [00:12<00:51, 15.36it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 200, Loss: -0.1946\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 30%|███       | 304/1000 [00:19<00:44, 15.52it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 300, Loss: -0.1948\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 40%|████      | 404/1000 [00:25<00:38, 15.32it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 400, Loss: -0.1985\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 50%|█████     | 502/1000 [00:32<00:36, 13.67it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 500, Loss: -0.2030\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 60%|██████    | 604/1000 [00:38<00:25, 15.57it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 600, Loss: -0.2040\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 70%|███████   | 704/1000 [00:45<00:19, 15.45it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 700, Loss: -0.2010\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 80%|████████  | 804/1000 [00:51<00:12, 15.57it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 800, Loss: -0.2063\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 90%|█████████ | 904/1000 [00:58<00:06, 15.84it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 900, Loss: -0.2074\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 1000/1000 [01:04<00:00, 15.56it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.5673828125\nSaving results...\nProcessing complete!\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    },
    {
     "execution_count": 29,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(NeuralHighlighter(\n   (mlp): ModuleList(\n     (0): Linear(in_features=3, out_features=256, bias=True)\n     (1): ReLU()\n     (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (3): Linear(in_features=256, out_features=256, bias=True)\n     (4): ReLU()\n     (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (6): Linear(in_features=256, out_features=256, bias=True)\n     (7): ReLU()\n     (8): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (9): Linear(in_features=256, out_features=256, bias=True)\n     (10): ReLU()\n     (11): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (12): Linear(in_features=256, out_features=256, bias=True)\n     (13): ReLU()\n     (14): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (15): Linear(in_features=256, out_features=256, bias=True)\n     (16): ReLU()\n     (17): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (18): Linear(in_features=256, out_features=2, bias=True)\n     (19): Softmax(dim=1)\n   )\n ),\n tensor([[ 0.1354, -0.3086, -0.2035],\n         [ 0.2923,  0.8583,  0.4218],\n         [-0.4693,  0.4590,  0.0762],\n         ...,\n         [ 0.1988,  0.4582, -0.2483],\n         [-0.0697,  0.0291, -0.0400],\n         [ 0.0098, -0.4411, -0.2057]], device='cuda:0'))"
     },
     "metadata": {}
    }
   ],
   "execution_count": 29
  }
 ]
}
