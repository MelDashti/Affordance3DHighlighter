{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#git clone with your token\n!git clone --branch extensions https://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@github.com/amiralichangizi/Affordance3DHighlighter.git","metadata":{"execution":{"iopub.status.busy":"2025-01-10T15:01:47.553917Z","iopub.execute_input":"2025-01-10T15:01:47.554263Z","iopub.status.idle":"2025-01-10T15:01:49.698758Z","shell.execute_reply.started":"2025-01-10T15:01:47.554236Z","shell.execute_reply":"2025-01-10T15:01:49.697522Z"},"id":"2rqJobuCpQLi","outputId":"67a93806-440e-4b7c-a56e-c43debd5ced5","trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'Affordance3DHighlighter'...\nremote: Enumerating objects: 360, done.\u001b[K\nremote: Counting objects: 100% (122/122), done.\u001b[K\nremote: Compressing objects: 100% (93/93), done.\u001b[K\nremote: Total 360 (delta 69), reused 67 (delta 29), pack-reused 238 (from 1)\u001b[K\nReceiving objects: 100% (360/360), 5.24 MiB | 13.30 MiB/s, done.\nResolving deltas: 100% (209/209), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!git pull origin extensions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:22:52.802401Z","iopub.execute_input":"2025-01-10T15:22:52.802785Z","iopub.status.idle":"2025-01-10T15:22:54.548463Z","shell.execute_reply.started":"2025-01-10T15:22:52.802752Z","shell.execute_reply":"2025-01-10T15:22:54.547610Z"}},"outputs":[{"name":"stdout","text":"remote: Enumerating objects: 8, done.\u001b[K\nremote: Counting objects: 100% (8/8), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nremote: Total 6 (delta 4), reused 4 (delta 2), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (6/6), 684 bytes | 97.00 KiB/s, done.\nFrom https://github.com/amiralichangizi/Affordance3DHighlighter\n * branch            extensions -> FETCH_HEAD\n   a4e0daa..105c0af  extensions -> origin/extensions\nUpdating a4e0daa..105c0af\nFast-forward\n extensions.ipynb | 7 \u001b[32m++++\u001b[m\u001b[31m---\u001b[m\n 1 file changed, 4 insertions(+), 3 deletions(-)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os\n\nos.chdir('/kaggle/working/Affordance3DHighlighter')","metadata":{"execution":{"iopub.status.busy":"2025-01-10T15:01:56.367304Z","iopub.execute_input":"2025-01-10T15:01:56.367597Z","iopub.status.idle":"2025-01-10T15:01:56.371827Z","shell.execute_reply.started":"2025-01-10T15:01:56.367574Z","shell.execute_reply":"2025-01-10T15:01:56.370796Z"},"id":"fhLYzi952EEA","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install gdown\n!gdown --id 1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\n!unzip full-shape.zip -d /kaggle/working/Affordance3DHighlighter/data/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:01:59.534343Z","iopub.execute_input":"2025-01-10T15:01:59.534634Z","iopub.status.idle":"2025-01-10T15:02:37.395339Z","shell.execute_reply.started":"2025-01-10T15:01:59.534612Z","shell.execute_reply":"2025-01-10T15:02:37.394197Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\nFrom (redirected): https://drive.google.com/uc?id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF&confirm=t&uuid=00531178-1f06-4546-a82b-e14c0dcf6b9d\nTo: /kaggle/working/Affordance3DHighlighter/full-shape.zip\n100%|████████████████████████████████████████| 558M/558M [00:07<00:00, 77.4MB/s]\nArchive:  full-shape.zip\n  inflating: /kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl  \n  inflating: /kaggle/working/Affordance3DHighlighter/data/full_shape_val_data.pkl  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pickle\nfrom collections import defaultdict\n\n# Load training data\nwith open('/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl', 'rb') as train_file:\n    dataset = pickle.load(train_file)\n    \n# Create a set to store unique semantic classes\nsemantic_classes = set()\nall_affordances = dataset[0]['affordance']\n\n# Iterate through the dataset and collect semantic classes\nfor item in dataset:\n    semantic_class = item['semantic class']\n    semantic_classes.add(semantic_class)\n   \n# Print all unique semantic classes\nprint(\"Unique semantic classes in the dataset:\")\nfor cls in sorted(semantic_classes):\n    print(f\"- {cls}\")\n\n# Print all unique semantic classes\nprint(\"Unique Affordances in the dataset:\")\nfor cls in sorted(all_affordances):\n    print(f\"- {cls}\")\n\n\n# Print total count\nprint(f\"\\nTotal number of unique semantic classes: {len(semantic_classes)}\")\nprint(f\"\\nTotal number of unique Affordances: {len(all_affordances)}\")\n\n# Optional: Print count of items per semantic class\nclass_counts = {}\nfor item in dataset:\n    semantic_class = item['semantic class']\n    class_counts[semantic_class] = class_counts.get(semantic_class, 0) + 1\n\nprint(\"\\nNumber of items per semantic class:\")\nfor cls, count in sorted(class_counts.items()):\n    print(f\"- {cls}: {count} items\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:46:27.445283Z","iopub.execute_input":"2025-01-10T15:46:27.445589Z","iopub.status.idle":"2025-01-10T15:46:30.064214Z","shell.execute_reply.started":"2025-01-10T15:46:27.445565Z","shell.execute_reply":"2025-01-10T15:46:30.063483Z"}},"outputs":[{"name":"stdout","text":"Unique semantic classes in the dataset:\n- Bag\n- Bed\n- Bottle\n- Bowl\n- Chair\n- Clock\n- Dishwasher\n- Display\n- Door\n- Earphone\n- Faucet\n- Hat\n- Keyboard\n- Knife\n- Laptop\n- Microwave\n- Mug\n- Refrigerator\n- Scissors\n- StorageFurniture\n- Table\n- TrashCan\n- Vase\nUnique Affordances in the dataset:\n- contain\n- cut\n- displaY\n- grasp\n- layable\n- lift\n- listen\n- move\n- openable\n- pourable\n- press\n- pull\n- pushable\n- sittable\n- stab\n- support\n- wear\n- wrap_grasp\n\nTotal number of unique semantic classes: 23\n\nTotal number of unique Affordances: 18\n\nNumber of items per semantic class:\n- Bag: 88 items\n- Bed: 127 items\n- Bottle: 288 items\n- Bowl: 132 items\n- Chair: 4280 items\n- Clock: 368 items\n- Dishwasher: 117 items\n- Display: 622 items\n- Door: 154 items\n- Earphone: 157 items\n- Faucet: 441 items\n- Hat: 156 items\n- Keyboard: 110 items\n- Knife: 225 items\n- Laptop: 295 items\n- Microwave: 130 items\n- Mug: 133 items\n- Refrigerator: 130 items\n- Scissors: 49 items\n- StorageFurniture: 1531 items\n- Table: 5593 items\n- TrashCan: 221 items\n- Vase: 735 items\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git\n!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html","metadata":{"id":"d8vCbctxbPP4","outputId":"303a2d90-d6d8-4bbd-97fa-621867d8c52c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport sys\nimport torch\n\nneed_pytorch3d = False\ntry:\n    import pytorch3d\nexcept ModuleNotFoundError:\n    need_pytorch3d = True\nif need_pytorch3d:\n    pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n    version_str = \"\".join([\n        f\"py3{sys.version_info.minor}_cu\",\n        torch.version.cuda.replace(\".\", \"\"),\n        f\"_pyt{pyt_version_str}\"\n    ])\n    !pip install iopath\n    if sys.platform.startswith(\"linux\"):\n        print(\"Trying to install wheel for PyTorch3D\")\n        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n        pip_list = !pip freeze\n        need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for i in pip_list)\n    if need_pytorch3d:\n        print(f\"failed to find/install wheel for {version_str}\")\nif need_pytorch3d:\n    print(\"Installing PyTorch3D from source\")\n    !pip install ninja\n    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'","metadata":{"ExecuteTime":{"end_time":"2024-12-26T14:58:53.585274Z","start_time":"2024-12-26T14:58:48.836644Z"},"execution":{"iopub.status.busy":"2025-01-10T15:03:30.363989Z","iopub.execute_input":"2025-01-10T15:03:30.364310Z","iopub.status.idle":"2025-01-10T15:03:45.952957Z","shell.execute_reply.started":"2025-01-10T15:03:30.364283Z","shell.execute_reply":"2025-01-10T15:03:45.952071Z"},"id":"sRNWfRMnIzuJ","outputId":"cbebc8f4-6572-4a67-a33e-060c1d89ce4e","trusted":true},"outputs":[{"name":"stdout","text":"Collecting iopath\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath) (4.66.5)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.12.2)\nCollecting portalocker (from iopath)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: iopath\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=ca0106d38b6955a3c80384b95df08378cb39f9a88e509540ca93e1e15605b92a\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built iopath\nInstalling collected packages: portalocker, iopath\nSuccessfully installed iopath-0.1.10 portalocker-3.1.1\nTrying to install wheel for PyTorch3D\nLooking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt241/download.html\nCollecting pytorch3d\n  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt241/pytorch3d-0.7.8-cp310-cp310-linux_x86_64.whl (20.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.10)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.66.5)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.12.2)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (3.1.1)\nInstalling collected packages: pytorch3d\nSuccessfully installed pytorch3d-0.7.8\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install --ignore-installed open3d","metadata":{"id":"zT9LfpcRXDHy","outputId":"a506d6d8-8390-403f-ac0b-c079ebdf6aee","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom src.mesh import Mesh\nfrom pytorch3d.structures import Pointclouds\n\nfrom src.convertor import obj_to_pointcloud\n\n\ndef bounding_sphere_normalize(points: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    points: (N,3) tensor of point coords\n    Return normalized points in a unit sphere centered at origin.\n    \"\"\"\n    center = points.mean(dim=0, keepdim=True)\n    max_dist = (points - center).norm(p=2, dim=1).max()\n    points_normed = (points - center) / max_dist\n    return points_normed\n\n\ndef load_3d_data(file_path, num_points=10000, device=\"cuda\", do_normalize=True):\n    \"\"\"\n    Loads 3D data as PyTorch3D Pointclouds from either NPZ point cloud or OBJ mesh.\n\n    Args:\n        file_path: Path to either .npz point cloud or .obj mesh file\n        num_points: Number of points to sample if loading from mesh\n        device: Device to load data on\n\n    Returns:\n        Pointclouds object containing points and features\n    \"\"\"\n    file_ext = file_path.split('.')[-1].lower()\n\n    if file_ext == 'npz':\n        # Load NPZ point cloud directly like in the example\n        pointcloud = np.load(file_path)\n        verts = torch.Tensor(pointcloud['verts']).to(device)\n        rgb = torch.Tensor(pointcloud['rgb']).to(device)\n\n        print(\"lenght of the data\")\n        print(len(verts))\n\n        # Subsample if needed\n        if len(verts) > num_points:\n            idx = torch.randperm(len(verts))[:num_points]\n            verts = verts[idx]\n            rgb = rgb[idx]\n\n        if do_normalize:\n            verts = bounding_sphere_normalize(verts)\n\n        # Return both the points tensor and the Pointclouds object\n        point_cloud = Pointclouds(points=[verts], features=[rgb])\n        return verts, point_cloud  # Return both\n\n    elif file_ext == 'obj':\n        # Load and convert your OBJ file\n        points, point_cloud = obj_to_pointcloud(\n            file_path,\n            num_points=num_points,  # Adjust this number as needed\n            device=\"cuda\"  # Use \"cpu\" if you don't have a GPU\n        )\n        if do_normalize:\n            points = bounding_sphere_normalize(points)\n            # here we update the point cloud too\n            rgb = point_cloud.features_packed() # shape [N,3]\n            point_cloud = Pointclouds(points = [points], features = [rgb])\n        return points, point_cloud\n       \n    else:\n        raise ValueError(f\"Unsupported file format: {file_ext}. Only .npz and .obj are supported.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-10T15:05:25.459408Z","iopub.execute_input":"2025-01-10T15:05:25.459761Z","iopub.status.idle":"2025-01-10T15:05:28.585774Z","shell.execute_reply.started":"2025-01-10T15:05:25.459727Z","shell.execute_reply":"2025-01-10T15:05:28.584885Z"},"id":"fxo1SjSH2NHm","outputId":"fd9c90bb-1f98-4e61-ee27-b43b8f1bbd64","trusted":true},"outputs":[{"name":"stdout","text":"Warp 1.5.1 initialized:\n   CUDA Toolkit 12.6, Driver 12.6\n   Devices:\n     \"cpu\"      : \"x86_64\"\n     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n     \"cuda:1\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n   CUDA peer access:\n     Supported fully (all-directional)\n   Kernel cache:\n     /root/.cache/warp/1.5.1\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from src.prompt_strategies import generate_affordance_prompt\nfrom src.evaluation_fullshapev2 import compute_mIoU\nfrom src.data_loader_fullshape import FullShapeDataset\nfrom src.render.cloud_point_renderer import MultiViewPointCloudRenderer\nfrom src.save_results import save_renders, save_results\nfrom src.neural_highlighter import NeuralHighlighter\nfrom src.Clip.loss_function import clip_loss\nfrom src.Clip.clip_model import get_clip_model, encode_text, setup_clip_transforms\n\nimport torch\nimport numpy as np\nimport random\nfrom tqdm import tqdm\n\n# Constrain most sources of randomness\n# (some torch backwards functions within CLIP are non-determinstic)\n# Set a consistent seed for reproducibility\nseed = 0  # You can use any integer value\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\n\ndef optimize_point_cloud(points, clip_model, renderer, encoded_text, log_dir: str, **kwargs):\n    num_iterations = kwargs.get('num_iterations', 1000)\n    learning_rate = kwargs.get('learning_rate', 1e-4)\n    depth = kwargs.get('depth', 5)\n    width = kwargs.get('network_width', 256)\n    n_views = kwargs.get(\"n_views\", 4)\n    n_augs = kwargs.get('n_augs', 1)\n    clipavg = kwargs.get('clipavg', 'view')\n    device = kwargs.get('device', 'cuda')\n    background_path = kwargs.get(\"background_path\",None)\n\n    # Initialize network and optimizer\n    net = NeuralHighlighter(\n        depth=depth,  # Number of hidden layers\n        width=width,  # Width of each layer\n        out_dim=2,  # Binary classification (highlight/no-highlight)\n        input_dim=3,  # 3D coordinates (x,y,z)\n        positional_encoding=False  # As recommended in the paper\n    ).to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\n    # Set up the transforms\n    clip_transform, augment_transform = setup_clip_transforms()\n\n    # Training loop\n    for i in tqdm(range(num_iterations)):\n        optimizer.zero_grad()\n\n        # Predict highlight probabilities\n        pred_class = net(points)\n\n        # Create colors based on predictions\n        highlight_color = torch.tensor([204 / 255, 1.0, 0.0]).to(device)\n        base_color = torch.tensor([180 / 255, 180 / 255, 180 / 255]).to(device)\n\n        colors = pred_class[:, 0:1] * highlight_color + pred_class[:, 1:2] * base_color\n\n        # Create and render point cloud\n        point_cloud = renderer.create_point_cloud(points, colors)\n        rendered_images = renderer.render_all_views(\n            point_cloud=point_cloud,\n            n_views=n_views,\n            background_path=background_path\n        )\n        # Convert dictionary of images to tensor\n        rendered_tensor = []\n        for name, img in rendered_images.items():\n            rendered_tensor.append(img.to(device))\n        rendered_tensor = torch.stack(rendered_tensor)\n\n        #Convert rendered images to CLIP format\n        rendered_images = rendered_tensor.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n        #print(rendered_images.shape)\n\n        # Calculate CLIP loss\n        loss = clip_loss(\n            rendered_images=rendered_images,\n            encoded_text=encoded_text,\n            clip_transform=clip_transform,\n            augment_transform=augment_transform,\n            clip_model=clip_model,\n            n_augs=n_augs,\n            clipavg=clipavg\n        )\n        #print(\"Loss computation graph:\")\n        #print_grad_fn(loss)\n        loss.backward()\n        optimizer.step()\n\n        if i % 100 == 0:\n            print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n            save_renders(log_dir, i, rendered_images)\n\n    return net\n\n\ndef main(input_path, object_name, highlight_region, **kwargs):\n    \"\"\"\n    Main function for 3D highlighting with configurable parameters.\n\n    Args:\n        input_path: Path to input 3D file (mesh or point cloud)\n        object_name: Name of the object for the prompt\n        highlight_region: Region to highlight\n        **kwargs: Optional parameters with defaults:\n            n_views: Number of views to render (default: 5)\n            n_aug: Number of augmentations (default: 5)\n            clipavg: Method for CLIP averaging (default: \"view\")\n            network_depth: Depth of neural network (default: 5)\n            network_width: Width of neural layers (default: 256)\n            learning_rate: Learning rate for optimization (default: 1e-4)\n            num_iterations: Number of training iterations (default: 500)\n            num_points: Number of points to sample (default: 10000)\n            device: Device to run on (default: \"cuda\")\n            output_dir: Directory for outputs (default: \"./output\")\n    \"\"\"\n    # Extract parameters from kwargs with defaults\n    n_views = kwargs.get(\"n_views\", 4)\n    num_points = kwargs.get(\"num_points\", 10000)\n    device = kwargs.get(\"device\", \"cuda\")\n    output_dir = kwargs.get(\"output_dir\", \"./output\")\n    do_normalize = kwargs.get(\"do_normalize\", True)\n    background_path = kwargs.get(\"background_path\", None)\n\n    try:\n        # Create output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n        \n        if input_path.split(\".\")[-1] == \"pkl\":\n            dataset = FullShapeDataset(input_path, device=device)\n            shape_entry = dataset[0]\n           \n            shape_class = shape_entry[\"shape_class\"]\n            affordance = shape_entry[\"affordances\"][0]\n            label_dict = shape_entry[\"labels_dict\"]\n\n            # Convert coords to tensor if not already\n            points = shape_entry[\"coords\"]\n            if not isinstance(points, torch.Tensor):\n                points = torch.tensor(points, device=device)\n\n            \n        else:\n            # Load 3D data (either mesh or point cloud)\n            print(f\"Loading 3D data from {input_path}...\")\n            points, point_cloud = load_3d_data(input_path, num_points=num_points, device=device)\n            print(f\"Loaded {len(points)} points\")\n\n        # Setup CLIP model\n        print(\"Setting up CLIP model...\")\n        clip_model, preprocess, resolution = get_clip_model()\n        \n        if input_path.split(\".\")[-1] == \"pkl\":\n            # Create and encode prompt\n            prompt = generate_affordance_prompt(shape_class, affordance, strategy=\"affordance_specific\")\n            print(f\"Using prompt: {prompt}\")\n            text_features = encode_text(clip_model, prompt, device)\n        else:\n            # Create and encode prompt\n            prompt = f\"A 3D render of a gray {object_name} with highlighted {highlight_region}\"\n            print(f\"Using prompt: {prompt}\")\n            text_features = encode_text(clip_model, prompt, device)\n            \n        # Initialize renderer\n        print(\"Setting up renderer...\")\n        renderer = MultiViewPointCloudRenderer(\n            camera_type=\"prespective\",\n            image_size=512,\n            base_dist=2.5,  # Your default view distance\n            base_elev=10,  # Your default elevation\n            base_azim=45,  # Your default azimuth\n            device=device,\n            point_radius=0.008\n        )\n        \n\n        # Optimize point cloud highlighting\n        print(\"Starting optimization...\")\n        net = optimize_point_cloud(\n            points=points,\n            renderer=renderer,\n            clip_model=clip_model,\n            encoded_text=text_features,\n            log_dir=output_dir,\n            **kwargs\n        )\n        \n        #Compute IoU for *this* affordance\n        with torch.no_grad():\n            pred_class = net(points)  # shape [N,2]\n            highlight_scores = pred_class[:, 0]\n            \n            gt_bin = (label_dict[affordance] > 0.0).long()\n            bin_pred = (highlight_scores >= 0.15).long()\n            iou_val = compute_mIoU(bin_pred, gt_bin)\n            print(f\"IoU: {iou_val}\")\n            \n\n        # Save results\n        print(\"Saving results...\")\n        save_results(\n            net=net,\n            points=points,\n            background_path=background_path,\n            n_views=n_views,\n            prompt=prompt,\n            output_dir=output_dir,\n            renderer=renderer,\n            device=device\n        )\n\n        print(\"Processing complete!\")\n        return net, points\n\n    except Exception as e:\n        print(f\"Error in processing: {str(e)}\")\n        raise\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-10T15:20:43.133975Z","iopub.execute_input":"2025-01-10T15:20:43.134340Z","iopub.status.idle":"2025-01-10T15:20:43.154153Z","shell.execute_reply.started":"2025-01-10T15:20:43.134312Z","shell.execute_reply":"2025-01-10T15:20:43.153263Z"},"id":"E0SBrmlBkwib","trusted":true,"jupyter":{"is_executing":true}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"sample_bg = \"/kaggle/working/Affordance3DHighlighter/data/background2.jpg\"\nmain(\n    input_path=\"/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl\",\n    object_name=\"dog\",\n    highlight_region=\"head\",\n    n_views=1,\n    n_augs=0,\n    clipavg=\"view\",\n    network_depth=4,\n    network_width=256,\n    learning_rate=1e-4,\n    num_iterations=1000,\n    num_points=100000,\n    device=\"cuda\",\n    output_dir=\"./output\",\n    background_path=None\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:40:14.969610Z","iopub.execute_input":"2025-01-10T15:40:14.970021Z","iopub.status.idle":"2025-01-10T15:41:31.395630Z","shell.execute_reply.started":"2025-01-10T15:40:14.969994Z","shell.execute_reply":"2025-01-10T15:41:31.394939Z"}},"outputs":[{"name":"stdout","text":"Found 1087 valid door objects with all affordances\nSetting up CLIP model...\nUsing prompt: A 3D render of a gray Vase emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nSetting up renderer...\nStarting optimization...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 2/1000 [00:00<01:17, 12.85it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 0, Loss: -0.1873\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 104/1000 [00:06<00:58, 15.35it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 100, Loss: -0.1873\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 202/1000 [00:12<00:51, 15.36it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 200, Loss: -0.1946\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 304/1000 [00:19<00:44, 15.52it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 300, Loss: -0.1948\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 404/1000 [00:25<00:38, 15.32it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 400, Loss: -0.1985\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 502/1000 [00:32<00:36, 13.67it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 500, Loss: -0.2030\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 604/1000 [00:38<00:25, 15.57it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 600, Loss: -0.2040\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 704/1000 [00:45<00:19, 15.45it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 700, Loss: -0.2010\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 804/1000 [00:51<00:12, 15.57it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 800, Loss: -0.2063\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 904/1000 [00:58<00:06, 15.84it/s]","output_type":"stream"},{"name":"stdout","text":"Iteration 900, Loss: -0.2074\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [01:04<00:00, 15.56it/s]","output_type":"stream"},{"name":"stdout","text":"IoU: 0.5673828125\nSaving results...\nProcessing complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(NeuralHighlighter(\n   (mlp): ModuleList(\n     (0): Linear(in_features=3, out_features=256, bias=True)\n     (1): ReLU()\n     (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (3): Linear(in_features=256, out_features=256, bias=True)\n     (4): ReLU()\n     (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (6): Linear(in_features=256, out_features=256, bias=True)\n     (7): ReLU()\n     (8): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (9): Linear(in_features=256, out_features=256, bias=True)\n     (10): ReLU()\n     (11): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (12): Linear(in_features=256, out_features=256, bias=True)\n     (13): ReLU()\n     (14): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (15): Linear(in_features=256, out_features=256, bias=True)\n     (16): ReLU()\n     (17): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (18): Linear(in_features=256, out_features=2, bias=True)\n     (19): Softmax(dim=1)\n   )\n ),\n tensor([[ 0.1354, -0.3086, -0.2035],\n         [ 0.2923,  0.8583,  0.4218],\n         [-0.4693,  0.4590,  0.0762],\n         ...,\n         [ 0.1988,  0.4582, -0.2483],\n         [-0.0697,  0.0291, -0.0400],\n         [ 0.0098, -0.4411, -0.2057]], device='cuda:0'))"},"metadata":{}}],"execution_count":29}]}