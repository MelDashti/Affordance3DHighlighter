{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:31:36.740485Z",
     "iopub.status.busy": "2025-01-07T22:31:36.740269Z",
     "iopub.status.idle": "2025-01-07T22:31:36.859276Z",
     "shell.execute_reply": "2025-01-07T22:31:36.858187Z",
     "shell.execute_reply.started": "2025-01-07T22:31:36.740466Z"
    },
    "id": "2rqJobuCpQLi",
    "outputId": "67a93806-440e-4b7c-a56e-c43debd5ced5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#git clone with your token\n",
    "!git clone https://github.com/amiralichangizi/Affordance3DHighlighter.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:31:39.867637Z",
     "iopub.status.busy": "2025-01-07T22:31:39.867346Z",
     "iopub.status.idle": "2025-01-07T22:31:39.871410Z",
     "shell.execute_reply": "2025-01-07T22:31:39.870517Z",
     "shell.execute_reply.started": "2025-01-07T22:31:39.867614Z"
    },
    "id": "fhLYzi952EEA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/kaggle/working/Affordance3DHighlighter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:31:43.342633Z",
     "iopub.status.busy": "2025-01-07T22:31:43.342322Z",
     "iopub.status.idle": "2025-01-07T22:32:14.365875Z",
     "shell.execute_reply": "2025-01-07T22:32:14.364693Z",
     "shell.execute_reply.started": "2025-01-07T22:31:43.342601Z"
    },
    "id": "OufUfyJM2NHk",
    "outputId": "3b5a0871-47ed-4dfa-deee-5bc4f10f98c1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown --id 1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\n",
    "!unzip full-shape.zip -d /kaggle/working/Affordance3DHighlighter/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T02:44:54.881392Z",
     "iopub.status.busy": "2025-01-07T02:44:54.881060Z",
     "iopub.status.idle": "2025-01-07T02:44:59.124382Z",
     "shell.execute_reply": "2025-01-07T02:44:59.123574Z",
     "shell.execute_reply.started": "2025-01-07T02:44:54.881360Z"
    },
    "id": "pE_BXtaR2NHl",
    "outputId": "2d3b4e5f-273a-4046-f755-36b530a160cd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load training data\n",
    "with open('/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl', 'rb') as train_file:\n",
    "    train_data = pickle.load(train_file)\n",
    "# Inspect the contents\n",
    "print(f\"Training Data Type: {type(train_data)}\")\n",
    "print(f\"Training Data Example: {train_data[:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:32:14.367698Z",
     "iopub.status.busy": "2025-01-07T22:32:14.367350Z",
     "iopub.status.idle": "2025-01-07T22:32:32.507372Z",
     "shell.execute_reply": "2025-01-07T22:32:32.506498Z",
     "shell.execute_reply.started": "2025-01-07T22:32:14.367663Z"
    },
    "id": "d8vCbctxbPP4",
    "outputId": "303a2d90-d6d8-4bbd-97fa-621867d8c52c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T14:58:53.585274Z",
     "start_time": "2024-12-26T14:58:48.836644Z"
    },
    "execution": {
     "iopub.execute_input": "2025-01-07T22:32:32.509149Z",
     "iopub.status.busy": "2025-01-07T22:32:32.508935Z",
     "iopub.status.idle": "2025-01-07T22:32:45.814689Z",
     "shell.execute_reply": "2025-01-07T22:32:45.813794Z",
     "shell.execute_reply.started": "2025-01-07T22:32:32.509130Z"
    },
    "id": "sRNWfRMnIzuJ",
    "outputId": "cbebc8f4-6572-4a67-a33e-060c1d89ce4e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "need_pytorch3d = False\n",
    "try:\n",
    "    import pytorch3d\n",
    "except ModuleNotFoundError:\n",
    "    need_pytorch3d = True\n",
    "if need_pytorch3d:\n",
    "    pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "    version_str = \"\".join([\n",
    "        f\"py3{sys.version_info.minor}_cu\",\n",
    "        torch.version.cuda.replace(\".\", \"\"),\n",
    "        f\"_pyt{pyt_version_str}\"\n",
    "    ])\n",
    "    !pip install iopath\n",
    "    if sys.platform.startswith(\"linux\"):\n",
    "        print(\"Trying to install wheel for PyTorch3D\")\n",
    "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "        pip_list = !pip freeze\n",
    "        need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for i in pip_list)\n",
    "    if need_pytorch3d:\n",
    "        print(f\"failed to find/install wheel for {version_str}\")\n",
    "if need_pytorch3d:\n",
    "    print(\"Installing PyTorch3D from source\")\n",
    "    !pip install ninja\n",
    "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:32:45.816733Z",
     "iopub.status.busy": "2025-01-07T22:32:45.816117Z",
     "iopub.status.idle": "2025-01-07T22:33:03.939324Z",
     "shell.execute_reply": "2025-01-07T22:33:03.938412Z",
     "shell.execute_reply.started": "2025-01-07T22:32:45.816709Z"
    },
    "id": "zT9LfpcRXDHy",
    "outputId": "a506d6d8-8390-403f-ac0b-c079ebdf6aee",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --ignore-installed open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T02:45:43.216305Z",
     "iopub.status.busy": "2025-01-07T02:45:43.216073Z",
     "iopub.status.idle": "2025-01-07T02:45:43.868769Z",
     "shell.execute_reply": "2025-01-07T02:45:43.867719Z",
     "shell.execute_reply.started": "2025-01-07T02:45:43.216286Z"
    },
    "id": "FTk44DGYYIBS",
    "outputId": "3a233b23-7245-4a71-e01a-168c07a440d9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/PittsburghBridge\n",
    "!wget -P data/PittsburghBridge https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:33:03.940801Z",
     "iopub.status.busy": "2025-01-07T22:33:03.940449Z",
     "iopub.status.idle": "2025-01-07T22:33:06.468610Z",
     "shell.execute_reply": "2025-01-07T22:33:06.467857Z",
     "shell.execute_reply.started": "2025-01-07T22:33:03.940769Z"
    },
    "id": "fxo1SjSH2NHm",
    "outputId": "fd9c90bb-1f98-4e61-ee27-b43b8f1bbd64",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from src.mesh import Mesh\n",
    "from pytorch3d.structures import Pointclouds\n",
    "\n",
    "from src.convertor import obj_to_pointcloud\n",
    "\n",
    "\n",
    "def bounding_sphere_normalize(points: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    points: (N,3) tensor of point coords\n",
    "    Return normalized points in a unit sphere centered at origin.\n",
    "    \"\"\"\n",
    "    center = points.mean(dim=0, keepdim=True)\n",
    "    max_dist = (points - center).norm(p=2, dim=1).max()\n",
    "    points_normed = (points - center) / max_dist\n",
    "    return points_normed\n",
    "\n",
    "\n",
    "def load_3d_data(file_path, num_points=10000, device=\"cuda\", do_normalize=True):\n",
    "    \"\"\"\n",
    "    Loads 3D data as PyTorch3D Pointclouds from either NPZ point cloud or OBJ mesh.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to either .npz point cloud or .obj mesh file\n",
    "        num_points: Number of points to sample if loading from mesh\n",
    "        device: Device to load data on\n",
    "\n",
    "    Returns:\n",
    "        Pointclouds object containing points and features\n",
    "    \"\"\"\n",
    "    file_ext = file_path.split('.')[-1].lower()\n",
    "\n",
    "    if file_ext == 'npz':\n",
    "        # Load NPZ point cloud directly like in the example\n",
    "        pointcloud = np.load(file_path)\n",
    "        verts = torch.Tensor(pointcloud['verts']).to(device)\n",
    "        rgb = torch.Tensor(pointcloud['rgb']).to(device)\n",
    "\n",
    "        print(\"lenght of the data\")\n",
    "        print(len(verts))\n",
    "\n",
    "        # Subsample if needed\n",
    "        if len(verts) > num_points:\n",
    "            idx = torch.randperm(len(verts))[:num_points]\n",
    "            verts = verts[idx]\n",
    "            rgb = rgb[idx]\n",
    "\n",
    "        if do_normalize:\n",
    "            verts = bounding_sphere_normalize(verts)\n",
    "\n",
    "        # Return both the points tensor and the Pointclouds object\n",
    "        point_cloud = Pointclouds(points=[verts], features=[rgb])\n",
    "        return verts, point_cloud  # Return both\n",
    "\n",
    "    elif file_ext == 'obj':\n",
    "        # Load and convert your OBJ file\n",
    "        points, point_cloud = obj_to_pointcloud(\n",
    "            file_path,\n",
    "            num_points=num_points,  # Adjust this number as needed\n",
    "            device=\"cuda\"  # Use \"cpu\" if you don't have a GPU\n",
    "        )\n",
    "        if do_normalize:\n",
    "            points = bounding_sphere_normalize(points)\n",
    "            # here we update the point cloud too\n",
    "            rgb = point_cloud.features_packed() # shape [N,3]\n",
    "            point_cloud = Pointclouds(points = [points], features = [rgb])\n",
    "        return points, point_cloud\n",
    "        # # Load mesh and sample points\n",
    "        # mesh = Mesh(file_path)\n",
    "        # vertices = mesh.vertices\n",
    "\n",
    "        # # Sample random points\n",
    "        # idx = torch.randperm(vertices.shape[0])[:num_points]\n",
    "        # points = vertices[idx].to(device)\n",
    "\n",
    "        # # Initialize with gray color\n",
    "        # colors = torch.ones_like(points) * 0.7\n",
    "\n",
    "        # return Pointclouds(points=[points], features=[colors])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_ext}. Only .npz and .obj are supported.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T02:46:57.302289Z",
     "iopub.status.busy": "2025-01-07T02:46:57.301968Z",
     "iopub.status.idle": "2025-01-07T02:46:57.306545Z",
     "shell.execute_reply": "2025-01-07T02:46:57.305757Z",
     "shell.execute_reply.started": "2025-01-07T02:46:57.302268Z"
    },
    "id": "kBamsUCJ2NHn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_grad_fn(tensor, depth=0):\n",
    "    \"\"\"Recursively print the gradient function graph\"\"\"\n",
    "    if tensor.grad_fn is None:\n",
    "        print(\"  \" * depth + \"None (leaf tensor)\")\n",
    "        return\n",
    "\n",
    "    print(\"  \" * depth + str(tensor.grad_fn))\n",
    "    for fn in tensor.grad_fn.next_functions:\n",
    "        if fn[0] is not None:\n",
    "            print(\"  \" * (depth + 1) + str(fn[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:33:06.469674Z",
     "iopub.status.busy": "2025-01-07T22:33:06.469364Z",
     "iopub.status.idle": "2025-01-07T22:33:08.698271Z",
     "shell.execute_reply": "2025-01-07T22:33:08.697262Z",
     "shell.execute_reply.started": "2025-01-07T22:33:06.469640Z"
    },
    "id": "E0SBrmlBkwib",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from src.render.cloud_point_renderer import MultiViewPointCloudRenderer\n",
    "from src.save_results import save_renders, save_results\n",
    "from src.neural_highlighter import NeuralHighlighter\n",
    "from src.Clip.loss_function import clip_loss\n",
    "from src.Clip.clip_model import get_clip_model, encode_text, setup_clip_transforms\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "# Set a consistent seed for reproducibility\n",
    "seed = 0  # You can use any integer value\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def optimize_point_cloud(points, clip_model, renderer, encoded_text, log_dir: str, **kwargs):\n",
    "    num_iterations = kwargs.get('num_iterations', 1000)\n",
    "    learning_rate = kwargs.get('learning_rate', 1e-4)\n",
    "    depth = kwargs.get('depth', 5)\n",
    "    width = kwargs.get('network_width', 256)\n",
    "    n_views = kwargs.get(\"n_views\", 4)\n",
    "    n_augs = kwargs.get('n_augs', 1)\n",
    "    clipavg = kwargs.get('clipavg', 'view')\n",
    "    device = kwargs.get('device', 'cuda')\n",
    "\n",
    "    # Initialize network and optimizer\n",
    "    net = NeuralHighlighter(\n",
    "        depth=depth,  # Number of hidden layers\n",
    "        width=width,  # Width of each layer\n",
    "        out_dim=2,  # Binary classification (highlight/no-highlight)\n",
    "        input_dim=3,  # 3D coordinates (x,y,z)\n",
    "        positional_encoding=False  # As recommended in the paper\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Set up the transforms\n",
    "    clip_transform, augment_transform = setup_clip_transforms()\n",
    "\n",
    "    # Training loop\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict highlight probabilities\n",
    "        pred_class = net(points)\n",
    "\n",
    "        # Create colors based on predictions\n",
    "        highlight_color = torch.tensor([204 / 255, 1.0, 0.0]).to(device)\n",
    "        base_color = torch.tensor([180 / 255, 180 / 255, 180 / 255]).to(device)\n",
    "\n",
    "        colors = pred_class[:, 0:1] * highlight_color + pred_class[:, 1:2] * base_color\n",
    "\n",
    "        # Create and render point cloud\n",
    "        point_cloud = renderer.create_point_cloud(points, colors)\n",
    "        rendered_images = renderer.render_all_views(point_cloud=point_cloud, n_views=n_views)\n",
    "        # Convert dictionary of images to tensor\n",
    "        rendered_tensor = []\n",
    "        for name, img in rendered_images.items():\n",
    "            rendered_tensor.append(img.to(device))\n",
    "        rendered_tensor = torch.stack(rendered_tensor)\n",
    "\n",
    "        #Convert rendered images to CLIP format\n",
    "        rendered_images = rendered_tensor.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
    "        #print(rendered_images.shape)\n",
    "\n",
    "        # Calculate CLIP loss\n",
    "        loss = clip_loss(\n",
    "            rendered_images=rendered_images,\n",
    "            encoded_text=encoded_text,\n",
    "            clip_transform=clip_transform,\n",
    "            augment_transform=augment_transform,\n",
    "            clip_model=clip_model,\n",
    "            n_augs=n_augs,\n",
    "            clipavg=clipavg\n",
    "        )\n",
    "        #print(\"Loss computation graph:\")\n",
    "        #print_grad_fn(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n",
    "            save_renders(log_dir, i, rendered_images)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def main(input_path, object_name, highlight_region, **kwargs):\n",
    "    \"\"\"\n",
    "    Main function for 3D highlighting with configurable parameters.\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to input 3D file (mesh or point cloud)\n",
    "        object_name: Name of the object for the prompt\n",
    "        highlight_region: Region to highlight\n",
    "        **kwargs: Optional parameters with defaults:\n",
    "            n_views: Number of views to render (default: 5)\n",
    "            n_aug: Number of augmentations (default: 5)\n",
    "            clipavg: Method for CLIP averaging (default: \"view\")\n",
    "            network_depth: Depth of neural network (default: 5)\n",
    "            network_width: Width of neural layers (default: 256)\n",
    "            learning_rate: Learning rate for optimization (default: 1e-4)\n",
    "            num_iterations: Number of training iterations (default: 500)\n",
    "            num_points: Number of points to sample (default: 10000)\n",
    "            device: Device to run on (default: \"cuda\")\n",
    "            output_dir: Directory for outputs (default: \"./output\")\n",
    "    \"\"\"\n",
    "    # Extract parameters from kwargs with defaults\n",
    "    n_views = kwargs.get(\"n_views\", 4)\n",
    "    num_points = kwargs.get(\"num_points\", 10000)\n",
    "    device = kwargs.get(\"device\", \"cuda\")\n",
    "    output_dir = kwargs.get(\"output_dir\", \"./output\")\n",
    "    do_normalize = kwargs.get(\"do_normalize\", True)\n",
    "\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Load 3D data (either mesh or point cloud)\n",
    "        print(f\"Loading 3D data from {input_path}...\")\n",
    "        points, point_cloud = load_3d_data(input_path, num_points=num_points, device=device)\n",
    "        print(f\"Loaded {len(points)} points\")\n",
    "\n",
    "        # Setup CLIP model\n",
    "        print(\"Setting up CLIP model...\")\n",
    "        clip_model, preprocess, resolution = get_clip_model()\n",
    "\n",
    "        # Create and encode prompt\n",
    "        prompt = f\"A 3D render of a gray {object_name} with highlighted {highlight_region}\"\n",
    "        print(f\"Using prompt: {prompt}\")\n",
    "        text_features = encode_text(clip_model, prompt, device)\n",
    "\n",
    "        # Initialize renderer\n",
    "        print(\"Setting up renderer...\")\n",
    "        renderer = MultiViewPointCloudRenderer(\n",
    "            image_size=512,\n",
    "            base_dist=30,  # Your default view distance\n",
    "            base_elev=10,  # Your default elevation\n",
    "            base_azim=0,  # Your default azimuth\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Optimize point cloud highlighting\n",
    "        print(\"Starting optimization...\")\n",
    "        net = optimize_point_cloud(\n",
    "            points=points,\n",
    "            renderer=renderer,\n",
    "            clip_model=clip_model,\n",
    "            encoded_text=text_features,\n",
    "            log_dir=output_dir,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Save results\n",
    "        print(\"Saving results...\")\n",
    "        save_results(\n",
    "            net=net,\n",
    "            points=points,\n",
    "            n_views=n_views,\n",
    "            prompt=prompt,\n",
    "            output_dir=output_dir,\n",
    "            renderer=renderer,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        print(\"Processing complete!\")\n",
    "        return net, points\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWM3ZR9_2NHo"
   },
   "source": [
    "### Evaluation for part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEKOPuSx2NHp"
   },
   "source": [
    "Ground truth visual representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:33:08.699962Z",
     "iopub.status.busy": "2025-01-07T22:33:08.699346Z",
     "iopub.status.idle": "2025-01-07T22:33:08.714202Z",
     "shell.execute_reply": "2025-01-07T22:33:08.713143Z",
     "shell.execute_reply.started": "2025-01-07T22:33:08.699926Z"
    },
    "id": "MIty92B94HEW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.save_results import save_point_cloud_results\n",
    "def save_ground_truth_results(data_entry, output_dir, renderer, device=\"cuda\", n_views=4):\n",
    "    \"\"\"\n",
    "    Saves visualization of ground truth affordance labels using existing save functions.\n",
    "    Now with more contrasting colors for better visibility.\n",
    "\n",
    "    Args:\n",
    "        data_entry: Dataset entry containing point cloud and labels\n",
    "        output_dir: Directory to save results\n",
    "        renderer: Point cloud renderer\n",
    "        device: Computing device\n",
    "        n_views: Number of views to render\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        points = data_entry[\"coords\"]\n",
    "        labels_dict = data_entry[\"labels_dict\"]\n",
    "        shape_class = data_entry[\"shape_class\"]\n",
    "\n",
    "        # Create ground truth directory\n",
    "        gt_dir = os.path.join(output_dir, 'ground_truth')\n",
    "        os.makedirs(gt_dir, exist_ok=True)\n",
    "\n",
    "        # Define colors for different affordances\n",
    "        affordance_colors = {\n",
    "            'openable': torch.tensor([1.0, 0.0, 0.0]).to(device)  # bright red\n",
    "            # 'pushable': torch.tensor([0.0, 0.0, 1.0]).to(device),  # bright blue\n",
    "            # 'pull': torch.tensor([0.0, 1.0, 0.0]).to(device)       # bright green\n",
    "        }\n",
    "\n",
    "        # Darker base color for better contrast\n",
    "        base_color = torch.tensor([0.3, 0.3, 0.3]).to(device)  # darker gray\n",
    "\n",
    "        # Process each affordance\n",
    "        for affordance, labels in labels_dict.items():\n",
    "            # Convert labels to the same format as network predictions\n",
    "            pred_class = torch.stack([1 - labels, labels], dim=1)\n",
    "\n",
    "            # Use the corresponding color for this affordance\n",
    "            highlight_color = affordance_colors.get(affordance, torch.tensor([1.0, 0.0, 0.0]).to(device))  # default to red\n",
    "            colors = pred_class[:, 1:2] * highlight_color + pred_class[:, 0:1] * base_color\n",
    "\n",
    "            # Save raw point cloud data using existing function\n",
    "            save_point_cloud_results(\n",
    "                points=points,\n",
    "                colors=colors,\n",
    "                output_path=os.path.join(gt_dir, f'gt_pointcloud_{affordance}.ply')\n",
    "            )\n",
    "\n",
    "            # Create and render point cloud\n",
    "            point_cloud = renderer.create_point_cloud(points, colors)\n",
    "            rendered_images = renderer.render_all_views(point_cloud=point_cloud, n_views=n_views,background_color=(0,0,0))\n",
    "\n",
    "            # Convert dictionary of images to tensor\n",
    "            rendered_tensor = []\n",
    "            for name, img in rendered_images.items():\n",
    "                rendered_tensor.append(img.to(device))\n",
    "            rendered_tensor = torch.stack(rendered_tensor)\n",
    "\n",
    "            # Convert rendered images to proper format\n",
    "            rendered_images = rendered_tensor.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
    "            rendered_images = (rendered_images * 255).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "            # Save rendered images using existing function\n",
    "            save_renders(\n",
    "                dir=gt_dir,\n",
    "                i=0,\n",
    "                rendered_images=rendered_images.float() / 255.0,\n",
    "                name=f'gt_render_{affordance}.png'\n",
    "            )\n",
    "\n",
    "            # Save metadata\n",
    "            with open(os.path.join(gt_dir, f'gt_info_{affordance}.txt'), 'w') as f:\n",
    "                f.write(f\"Shape Class: {shape_class}\\n\")\n",
    "                f.write(f\"Affordance: {affordance}\\n\")\n",
    "                f.write(f\"Positive labels: {labels.sum().item()}/{len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fdoEbMQ2NHp"
   },
   "source": [
    "### Part 3\n",
    "Hyperparam + Strategy Tuning, Then Test Evaluation\n",
    "Teammates focus on the below code block for the reporting part of part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T22:33:09.418974Z",
     "iopub.status.busy": "2025-01-07T22:33:09.418613Z",
     "iopub.status.idle": "2025-01-07T22:33:10.428896Z",
     "shell.execute_reply": "2025-01-07T22:33:10.427827Z",
     "shell.execute_reply.started": "2025-01-07T22:33:09.418943Z"
    },
    "id": "vFI0Nlrz2NHp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.prompt_strategies import generate_affordance_prompt\n",
    "\n",
    "\n",
    "def grid_search_validation(\n",
    "    val_dataset, # validation set\n",
    "    clip_model,  # clip model\n",
    "    device='cuda',\n",
    "    strategies=('affordance_specific'),\n",
    "    thresholds=[0.3],\n",
    "    learning_rates=(0.01, 0.001), # learning rates\n",
    "    depths=(4,5),\n",
    "    num_augs=(1,3),\n",
    "    num_views=(2,3),\n",
    "    num_val_objects=5,  # number of validation objects to use\n",
    "    num_iterations=500,\n",
    "    output_dir=\"./val_gridsearch\"\n",
    "):\n",
    "    \"\"\"\n",
    "    For each (strategy, threshold), pick up to 'num_val_objects' shapes from val_dataset,\n",
    "    train & evaluate *per affordance* => average IoU => pick best combo.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import os\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    val_indices = list(range(min(num_val_objects, len(val_dataset))))\n",
    "    best_strategy = None\n",
    "    best_threshold = None\n",
    "    best_learning_rate = None\n",
    "    best_depth = None\n",
    "    best_num_augs = None\n",
    "    best_num_views = None\n",
    "    best_iou = -1.0\n",
    "    all_results = []\n",
    "\n",
    "    print(\"[grid_search_validation] Starting shape-by-shape (per-affordance) training on validation...\")\n",
    "\n",
    "    for strategy in strategies:\n",
    "        for th in thresholds:\n",
    "            for lr in learning_rates:  # assuming you're testing a fixed learning rate\n",
    "                for depth in depths:  # assuming you're testing a fixed depth\n",
    "                    for num_aug in num_augs:  # assuming you're testing a fixed number of augmentations\n",
    "                        for num_view in num_views:  # assuming you're testing a fixed number of views\n",
    "                            print(f\"  Trying strategy='{strategy}' threshold={th} lr={lr} depth={depth} num_aug={num_aug} num_view={num_view}\")\n",
    "\n",
    "                            sum_iou = 0.0\n",
    "                            count_shapes = 0\n",
    "\n",
    "                            combo_dir = os.path.join(output_dir, f\"val_{strategy}_th{th}_lr{lr}_depth_{depth}_n_aug_{num_aug}_n_views_{num_view}\")\n",
    "                            os.makedirs(combo_dir, exist_ok=True)\n",
    "\n",
    "                            # For each shape in val_indices\n",
    "                            for i, idx in enumerate(val_indices):\n",
    "                                shape_entry = val_dataset[idx]\n",
    "                                shape_id = shape_entry.get('shape_id', f'valshape_{idx}')\n",
    "                                shape_class = shape_entry[\"shape_class\"]\n",
    "                                affs = shape_entry[\"affordances\"]\n",
    "                                label_dict = shape_entry[\"labels_dict\"]\n",
    "\n",
    "                                # Convert coords to tensor if not already\n",
    "                                shape_coords = shape_entry[\"coords\"]\n",
    "                                if not isinstance(shape_coords, torch.Tensor):\n",
    "                                    shape_coords = torch.tensor(shape_coords, device=device)\n",
    "\n",
    "                                shape_subdir = os.path.join(combo_dir, f\"val_shape_{shape_id}\")\n",
    "                                os.makedirs(shape_subdir, exist_ok=True)\n",
    "\n",
    "                                print(f\"  -> [Shape {i+1}/{len(val_indices)}] ID={shape_id}, Class={shape_class}, Affs={affs}\")\n",
    "\n",
    "                                try:\n",
    "                                    # 1) Save ground truth visualization\n",
    "                                    renderer = MultiViewPointCloudRenderer(\n",
    "                                        image_size=256, base_dist=20, base_elev=10, device=device\n",
    "                                    )\n",
    "                                    save_ground_truth_results(\n",
    "                                        data_entry=shape_entry,\n",
    "                                        output_dir=shape_subdir,\n",
    "                                        renderer=renderer,\n",
    "                                        device=device,\n",
    "                                        n_views=4\n",
    "                                    )\n",
    "\n",
    "                                    # 2) Train & evaluate *separately* for each affordance\n",
    "                                    shape_sum_iou = 0.0\n",
    "\n",
    "                                    for aff in affs:\n",
    "                                        # Create a sub-folder for this affordance’s run\n",
    "                                        aff_subdir = os.path.join(shape_subdir, f\"aff_{aff}\")\n",
    "                                        os.makedirs(aff_subdir, exist_ok=True)\n",
    "                                        os.makedirs(os.path.join(aff_subdir, \"predictions\"), exist_ok=True)\n",
    "\n",
    "\n",
    "                                        # (b) Prompt for *this* affordance\n",
    "                                        prompt_str = generate_affordance_prompt(shape_class, aff, strategy)\n",
    "                                        text_feats = encode_text(clip_model, prompt_str, device=device)\n",
    "\n",
    "                                        # (c) Optimize with the given number of views and learning rate\n",
    "                                        net = optimize_point_cloud(\n",
    "                                            learning_rate=lr,\n",
    "                                            depth = depth,\n",
    "                                            points=shape_coords,\n",
    "                                            clip_model=clip_model,\n",
    "                                            renderer=renderer,\n",
    "                                            encoded_text=text_feats,\n",
    "                                            log_dir=os.path.join(aff_subdir, \"optimization_logs\"),\n",
    "                                            num_iterations=num_iterations,\n",
    "                                            device=device,\n",
    "                                            n_augs=num_aug,\n",
    "                                            n_views=num_view  # Using the num_views hyperparameter here\n",
    "                                        )\n",
    "\n",
    "                                        # (d) Save predictions\n",
    "                                        save_results(\n",
    "                                            net=net,\n",
    "                                            points=shape_coords,\n",
    "                                            prompt=prompt_str,\n",
    "                                            output_dir=os.path.join(aff_subdir, \"predictions\"),\n",
    "                                            renderer=renderer,\n",
    "                                            device=device,\n",
    "                                            n_views=num_view  # Using the num_views hyperparameter here\n",
    "                                        )\n",
    "\n",
    "                                        # (e) Compute IoU for *this* affordance\n",
    "                                        with torch.no_grad():\n",
    "                                            pred_class = net(shape_coords)  # shape [N,2]\n",
    "                                            highlight_scores = pred_class[:, 0]\n",
    "\n",
    "                                        gt_bin = (label_dict[aff] > 0.0).long()\n",
    "                                        bin_pred = (highlight_scores >= th).long()\n",
    "                                        iou_val = compute_mIoU(bin_pred, gt_bin)\n",
    "\n",
    "                                        # Save a quick metrics file for this affordance\n",
    "                                        with open(os.path.join(aff_subdir, \"metrics.txt\"), 'w') as f:\n",
    "                                            f.write(f\"Validation shape ID: {shape_id}\\n\")\n",
    "                                            f.write(f\"Affordance: {aff}\\n\")\n",
    "                                            f.write(f\"Shape Class: {shape_class}\\n\")\n",
    "                                            f.write(f\"Prompt strategy: {strategy}\\n\")\n",
    "                                            f.write(f\"LR: {lr}\\n\")\n",
    "                                            f.write(f\"Depth: {depth}\\n\")\n",
    "                                            f.write(f\"Num Augmentations: {num_aug}\\n\")\n",
    "                                            f.write(f\"Num Views: {num_view}\\n\")\n",
    "                                            f.write(f\"Threshold: {th}\\n\")\n",
    "                                            f.write(f\"IoU: {iou_val:.3f}\\n\")\n",
    "\n",
    "                                        shape_sum_iou += iou_val\n",
    "\n",
    "                                    # (f) Average IoU across all affs for this shape\n",
    "                                    shape_mean_iou = shape_sum_iou / len(affs) if len(affs) > 0 else 0.0\n",
    "\n",
    "                                    # Summation for this (strategy, threshold) combo\n",
    "                                    sum_iou += shape_mean_iou\n",
    "                                    count_shapes += 1\n",
    "\n",
    "                                    # Write shape-level metrics\n",
    "                                    with open(os.path.join(shape_subdir, \"metrics_summary.txt\"), 'w') as f:\n",
    "                                        f.write(f\"Validation shape ID: {shape_id}\\n\")\n",
    "                                        f.write(f\"All affordances: {affs}\\n\")\n",
    "                                        f.write(f\"Final shape mean IoU (averaged over {len(affs)} affs) = {shape_mean_iou:.3f}\\n\")\n",
    "\n",
    "                                    print(f\"     Shape mean IoU => {shape_mean_iou:.3f}\")\n",
    "\n",
    "                                except Exception as e:\n",
    "                                    print(f\"[Warning] Skipped shape idx={idx} due to error: {e}\")\n",
    "                                    continue\n",
    "\n",
    "                            # After finishing these shapes for this (strategy, threshold) combo...\n",
    "                            avg_iou = sum_iou / count_shapes if count_shapes > 0 else 0.0\n",
    "                            all_results.append((strategy, th, avg_iou))\n",
    "                            print(f\"\\n>>> Combo (strategy='{strategy}' threshold={th} lr={lr} depth={depth} num_aug={num_aug} num_view={num_view}) => Mean IoU={avg_iou:.3f} over {count_shapes} shapes\")\n",
    "\n",
    "                            # Track best\n",
    "                            if avg_iou > best_iou:\n",
    "                                best_iou = avg_iou\n",
    "                                best_strategy = strategy\n",
    "                                best_threshold = th\n",
    "                                best_learning_rate = lr\n",
    "                                best_depth = depth\n",
    "                                best_num_aug = num_aug\n",
    "                                best_num_view = num_view\n",
    "\n",
    "    print(\"\\n[grid_search_validation] Validation combos sorted by best IoU:\")\n",
    "    sorted_res = sorted(all_results, key=lambda x: x[2], reverse=True)\n",
    "    for (s, t, iou) in sorted_res:\n",
    "        print(f\"    Strategy={s}, Th={t}, Lr={lr}, Depth={depth}, Num_Aug={num_aug}, Num_Views={num_view}, IoU={iou:.3f}\")\n",
    "\n",
    "    print(f\"\\n[grid_search_validation] Best strategy={best_strategy}, threshold={best_threshold}, learning_rate={best_learning_rate}, depth={best_depth}, num_views={best_num_view}, num_augs={best_num_aug}, average mIoU={best_iou:.3f}\")\n",
    "    return best_strategy, best_threshold, best_learning_rate, best_depth, best_num_aug, best_num_view, best_iou\n",
    "\n",
    "def test_phase_evaluation(\n",
    "    test_dataset,\n",
    "    clip_model,\n",
    "    best_strategy,\n",
    "    best_threshold,\n",
    "    best_lr,\n",
    "    best_depth,\n",
    "    best_num_augs,\n",
    "    best_num_views,\n",
    "    device='cuda',\n",
    "    num_test_shapes=5,\n",
    "    num_iterations=500,\n",
    "    output_dir=\"./test_results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the pipeline on the test dataset, *per affordance*.\n",
    "    Returns the final average test IoU (averaged across shapes and affordances).\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import random\n",
    "\n",
    "    test_indices = list(range(min(num_test_shapes, len(test_dataset))))\n",
    "    sum_test_iou = 0.0\n",
    "    count_shapes = 0\n",
    "\n",
    "    print(f\"[test_phase_evaluation] Using strategy={best_strategy}, threshold={best_threshold}\")\n",
    "\n",
    "    for idx in test_indices:\n",
    "        shape_entry = test_dataset[idx]\n",
    "        shape_id = shape_entry.get('shape_id', f'shape_{idx}')\n",
    "        shape_class = shape_entry[\"shape_class\"]\n",
    "        affs = shape_entry[\"affordances\"]\n",
    "        label_dict = shape_entry[\"labels_dict\"]\n",
    "\n",
    "        shape_dir = os.path.join(output_dir, f\"test_shape_{shape_id}\")\n",
    "        os.makedirs(shape_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"  -> [Test Shape] ID={shape_id}, Class={shape_class}, Affs={affs}\")\n",
    "\n",
    "        try:\n",
    "            # 1) Save ground truth visualization\n",
    "            renderer = MultiViewPointCloudRenderer(\n",
    "                image_size=256, base_dist=20, base_elev=10, device=device\n",
    "            )\n",
    "            save_ground_truth_results(\n",
    "                data_entry=shape_entry,\n",
    "                output_dir=shape_dir,\n",
    "                renderer=renderer,\n",
    "                device=device,\n",
    "                n_views=best_num_views\n",
    "            )\n",
    "\n",
    "            shape_coords = shape_entry[\"coords\"]\n",
    "            if not isinstance(shape_coords, torch.Tensor):\n",
    "                shape_coords = torch.tensor(shape_coords, device=device)\n",
    "\n",
    "            # 2) For each affordance, train a new network and compute IoU\n",
    "            shape_sum_iou = 0.0\n",
    "\n",
    "            for aff in affs:\n",
    "                aff_dir = os.path.join(shape_dir, f\"aff_{aff}\")\n",
    "                os.makedirs(aff_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "                # (b) Prompt\n",
    "                prompt_str = generate_affordance_prompt(shape_class, aff, best_strategy)\n",
    "                text_feats = encode_text(clip_model, prompt_str, device=device)\n",
    "\n",
    "                # (c) Optimize\n",
    "                net = optimize_point_cloud(\n",
    "                    points=shape_coords,\n",
    "                    depth = best_depth,\n",
    "                    learning_rate=best_lr,\n",
    "                    clip_model=clip_model,\n",
    "                    renderer=renderer,\n",
    "                    encoded_text=text_feats,\n",
    "                    log_dir=os.path.join(aff_dir, \"optimization_logs\"),\n",
    "                    num_iterations=num_iterations,\n",
    "                    device=device,\n",
    "                    n_augs=best_num_augs,\n",
    "                    n_views=best_num_views\n",
    "                )\n",
    "\n",
    "                # (d) Save predictions\n",
    "                os.makedirs(os.path.join(aff_dir, \"predictions\"), exist_ok=True)\n",
    "                save_results(\n",
    "                    net=net,\n",
    "                    points=shape_coords,\n",
    "                    prompt=prompt_str,\n",
    "                    output_dir=os.path.join(aff_dir, \"predictions\"),\n",
    "                    renderer=renderer,\n",
    "                    device=device,\n",
    "                    n_views=best_num_views\n",
    "                )\n",
    "\n",
    "                # (e) Compute IoU for this aff\n",
    "                with torch.no_grad():\n",
    "                    pred_class = net(shape_coords)\n",
    "                    highlight_scores = pred_class[:, 0]\n",
    "\n",
    "                gt_bin = (label_dict[aff] > 0.0).long()\n",
    "                bin_pred = (highlight_scores >= best_threshold).long()\n",
    "                iou_val = compute_mIoU(bin_pred, gt_bin)\n",
    "                shape_sum_iou += iou_val\n",
    "\n",
    "                # Save per-affordance metrics\n",
    "                with open(os.path.join(aff_dir, \"metrics.txt\"), 'w') as f:\n",
    "                    f.write(f\"Test shape ID: {shape_id}\\n\")\n",
    "                    f.write(f\"Shape Class: {shape_class}\\n\")\n",
    "                    f.write(f\"Affordance: {aff}\\n\")\n",
    "                    f.write(f\"Prompt strategy: {best_strategy}\\n\")\n",
    "                    f.write(f\"Threshold: {best_threshold}\\n\")\n",
    "                    f.write(f\"Depth: {best_depth}\\n\")\n",
    "                    f.write(f\"Augmentations: {best_num_augs}\\n\")\n",
    "                    f.write(f\"Number of Views: {best_num_views}\\n\")\n",
    "                    f.write(f\"IoU: {iou_val:.3f}\\n\")\n",
    "\n",
    "            # 3) Average IoU across this shape’s affordances\n",
    "            shape_mean = shape_sum_iou / len(affs) if len(affs) > 0 else 0.0\n",
    "            sum_test_iou += shape_mean\n",
    "            count_shapes += 1\n",
    "\n",
    "            # Write shape-level summary\n",
    "            with open(os.path.join(shape_dir, \"metrics_summary.txt\"), 'w') as f:\n",
    "                f.write(f\"Shape ID: {shape_id}\\n\")\n",
    "                f.write(f\"Class: {shape_class}\\n\")\n",
    "                f.write(f\"Affordances: {affs}\\n\")\n",
    "                f.write(f\"Strategy: {best_strategy}\\n\")\n",
    "                f.write(f\"Threshold: {best_threshold}\\n\")\n",
    "                f.write(f\"Augmentations: {best_num_augs}\\n\")\n",
    "                f.write(f\"Depth: {best_depth}\\n\")\n",
    "                f.write(f\"Number of Views: {best_num_views}\\n\")\n",
    "                f.write(f\"Mean IoU (this shape): {shape_mean:.3f}\\n\")\n",
    "\n",
    "            print(f\"     shape mean IoU => {shape_mean:.3f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [Warning] Skipped shape idx={idx} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 4) Final average over all tested shapes\n",
    "    final_test_iou = sum_test_iou / count_shapes if count_shapes > 0 else 0.0\n",
    "    print(f\"[test_phase_evaluation] Final mIoU = {final_test_iou:.3f} (over {count_shapes} shapes)\")\n",
    "\n",
    "    # Save overall test results\n",
    "    with open(os.path.join(output_dir, \"test_summary.txt\"), 'w') as f:\n",
    "        f.write(\"Test Summary\\n\")\n",
    "        f.write(\"============\\n\")\n",
    "        f.write(f\"Number of shapes tested: {count_shapes}\\n\")\n",
    "        f.write(f\"Best strategy: {best_strategy}\\n\")\n",
    "        f.write(f\"Best threshold: {best_threshold}\\n\")\n",
    "        f.write(f\"Final mIoU: {final_test_iou:.3f}\\n\")\n",
    "\n",
    "    return final_test_iou\n",
    "\n",
    "\n",
    "def compute_mIoU(pred_labels, gt_labels):\n",
    "    intersection = ((pred_labels == 1) & (gt_labels == 1)).sum().float()\n",
    "    union = ((pred_labels == 1) | (gt_labels == 1)).sum().float()\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return (intersection / union).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeqFmgmU2NHq"
   },
   "source": [
    "## Here we pass different strategies and threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-07T22:59:01.979Z",
     "iopub.execute_input": "2025-01-07T22:33:16.739956Z",
     "iopub.status.busy": "2025-01-07T22:33:16.739642Z"
    },
    "id": "rCVuykp52NHq",
    "outputId": "84406ac4-7786-4f92-e5b9-9c91ddc60de0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Imports from your code\n",
    "from src.data_loader_fullshape import create_dataset_splits, FullShapeDataset\n",
    "from src.prompt_strategies import generate_affordance_prompt\n",
    "from src.Clip.clip_model import get_clip_model\n",
    "from src.render.cloud_point_renderer import MultiViewPointCloudRenderer\n",
    "from src.neural_highlighter import NeuralHighlighter\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "dataset = FullShapeDataset(\"/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl\", device=device)\n",
    "train_data, val_data, test_data = create_dataset_splits(dataset, val_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "\n",
    "print(f\"[Main] val_data size={len(val_data)}, test_data size={len(test_data)}\")\n",
    "\n",
    "# 3) Load CLIP model\n",
    "clip_model, _, _ = get_clip_model(\"ViT-L/14\")\n",
    "\n",
    "\n",
    "best_strat, best_th,best_lr, best_depth, best_num_aug, best_num_view, best_val_iou,  = grid_search_validation(\n",
    "    val_dataset=val_data,\n",
    "    clip_model=clip_model,\n",
    "    device=device,\n",
    "    output_dir=\"./val_gridsearch_07012025\"\n",
    ")\n",
    "\n",
    "print(f\"\\n[Main] Validation done => best strategy={best_strat}, threshold={best_th}, LR={best_lr} , depth={best_depth}, num_aug={best_num_aug}, num_views={best_num_view}    valIoU={best_val_iou:.3f}\")\n",
    "\n",
    "# 5) Test Phase\n",
    "num_test_shapes = 5\n",
    "test_iterations = 500\n",
    "final_test_iou = test_phase_evaluation(\n",
    "    test_dataset=test_data,\n",
    "    clip_model=clip_model,\n",
    "    best_strategy=best_strat,\n",
    "    best_threshold=best_th,\n",
    "    best_lr=best_lr,\n",
    "    best_depth=best_depth,\n",
    "    best_num_augs=best_num_aug,\n",
    "    best_num_views=best_num_view,\n",
    "    device=device,\n",
    "    num_test_shapes=num_test_shapes,\n",
    "    num_iterations=test_iterations,\n",
    "    output_dir=\"./test_results_07012025\"\n",
    "\n",
    " )\n",
    "print(f\"[Main] Final test IoU => {final_test_iou:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T13:21:53.463353Z",
     "iopub.status.busy": "2025-01-08T13:21:53.463045Z",
     "iopub.status.idle": "2025-01-08T13:21:53.582281Z",
     "shell.execute_reply": "2025-01-08T13:21:53.581171Z",
     "shell.execute_reply.started": "2025-01-08T13:21:53.463331Z"
    },
    "id": "brtGq1c71lzT",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tzip warning: name not matched: /kaggle/working/Affordance3DHighlighter/test_results_07012025\n",
      "\tzip warning: name not matched: /kaggle/working/Affordance3DHighlighter/val_gridsearch_07012025\n",
      "\n",
      "zip error: Nothing to do! (try: zip -r output_07012025.zip . -i /kaggle/working/Affordance3DHighlighter/test_results_07012025 /kaggle/working/Affordance3DHighlighter/val_gridsearch_07012025)\n"
     ]
    }
   ],
   "source": [
    "!zip -r output_07012025.zip /kaggle/working/Affordance3DHighlighter/test_results_07012025 /kaggle/working/Affordance3DHighlighter/val_gridsearch_07012025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2d89gC-1lzT",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
