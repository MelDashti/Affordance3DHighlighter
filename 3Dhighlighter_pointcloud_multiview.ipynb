{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-29T13:22:56.405362Z",
     "iopub.status.busy": "2024-12-29T13:22:56.405013Z",
     "iopub.status.idle": "2024-12-29T13:22:57.472712Z",
     "shell.execute_reply": "2024-12-29T13:22:57.471524Z",
     "shell.execute_reply.started": "2024-12-29T13:22:56.405331Z"
    },
    "id": "2rqJobuCpQLi",
    "outputId": "281b69ef-7d5c-40e7-e9d1-c343706616ea",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T13:23:00.657572Z",
     "iopub.status.busy": "2024-12-29T13:23:00.657241Z",
     "iopub.status.idle": "2024-12-29T13:23:00.661274Z",
     "shell.execute_reply": "2024-12-29T13:23:00.660466Z",
     "shell.execute_reply.started": "2024-12-29T13:23:00.657544Z"
    },
    "id": "fhLYzi952EEA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/kaggle/working/Affordance3DHighlighter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown --id 1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\n",
    "!unzip full-shape.zip -d /kaggle/working/Affordance3DHighlighter/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load training data\n",
    "with open('/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl', 'rb') as train_file:\n",
    "    train_data = pickle.load(train_file)\n",
    "# Inspect the contents\n",
    "print(f\"Training Data Type: {type(train_data)}\")\n",
    "print(f\"Training Data Example: {train_data[:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-29T13:24:23.588989Z",
     "iopub.status.busy": "2024-12-29T13:24:23.588319Z",
     "iopub.status.idle": "2024-12-29T13:24:31.831852Z",
     "shell.execute_reply": "2024-12-29T13:24:31.830950Z",
     "shell.execute_reply.started": "2024-12-29T13:24:23.588955Z"
    },
    "id": "d8vCbctxbPP4",
    "outputId": "55a100ed-7b85-4630-9e3f-e3ce98c37fff",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T14:58:53.585274Z",
     "start_time": "2024-12-26T14:58:48.836644Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-29T14:25:20.704313Z",
     "iopub.status.busy": "2024-12-29T14:25:20.703998Z",
     "iopub.status.idle": "2024-12-29T14:25:20.715992Z",
     "shell.execute_reply": "2024-12-29T14:25:20.715196Z",
     "shell.execute_reply.started": "2024-12-29T14:25:20.704290Z"
    },
    "id": "sRNWfRMnIzuJ",
    "outputId": "39ed1f77-3f8e-499d-eb2b-8aecadf7c335",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "need_pytorch3d = False\n",
    "try:\n",
    "    import pytorch3d\n",
    "except ModuleNotFoundError:\n",
    "    need_pytorch3d = True\n",
    "if need_pytorch3d:\n",
    "    pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "    version_str = \"\".join([\n",
    "        f\"py3{sys.version_info.minor}_cu\",\n",
    "        torch.version.cuda.replace(\".\", \"\"),\n",
    "        f\"_pyt{pyt_version_str}\"\n",
    "    ])\n",
    "    !pip install iopath\n",
    "    if sys.platform.startswith(\"linux\"):\n",
    "        print(\"Trying to install wheel for PyTorch3D\")\n",
    "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "        pip_list = !pip freeze\n",
    "        need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for i in pip_list)\n",
    "    if need_pytorch3d:\n",
    "        print(f\"failed to find/install wheel for {version_str}\")\n",
    "if need_pytorch3d:\n",
    "    print(\"Installing PyTorch3D from source\")\n",
    "    !pip install ninja\n",
    "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-29T13:25:35.005268Z",
     "iopub.status.busy": "2024-12-29T13:25:35.004942Z",
     "iopub.status.idle": "2024-12-29T13:25:52.711356Z",
     "shell.execute_reply": "2024-12-29T13:25:52.710373Z",
     "shell.execute_reply.started": "2024-12-29T13:25:35.005248Z"
    },
    "id": "zT9LfpcRXDHy",
    "outputId": "527d847c-60da-4e72-c096-81cbf9671f92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-29T13:17:38.518795Z",
     "iopub.status.busy": "2024-12-29T13:17:38.518502Z",
     "iopub.status.idle": "2024-12-29T13:17:38.949273Z",
     "shell.execute_reply": "2024-12-29T13:17:38.948438Z",
     "shell.execute_reply.started": "2024-12-29T13:17:38.518773Z"
    },
    "id": "FTk44DGYYIBS",
    "outputId": "864b8bfd-a638-4f09-f520-d3f0fb75389c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/PittsburghBridge\n",
    "!wget -P data/PittsburghBridge https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T14:25:27.631711Z",
     "iopub.status.busy": "2024-12-29T14:25:27.631412Z",
     "iopub.status.idle": "2024-12-29T14:25:27.639712Z",
     "shell.execute_reply": "2024-12-29T14:25:27.638779Z",
     "shell.execute_reply.started": "2024-12-29T14:25:27.631688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from src.mesh import Mesh\n",
    "from pytorch3d.structures import Pointclouds\n",
    "\n",
    "from src.convertor import obj_to_pointcloud\n",
    "\n",
    "\n",
    "def bounding_sphere_normalize(points: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    points: (N,3) tensor of point coords\n",
    "    Return normalized points in a unit sphere centered at origin.\n",
    "    \"\"\"\n",
    "    center = points.mean(dim=0, keepdim=True)\n",
    "    max_dist = (points - center).norm(p=2, dim=1).max()\n",
    "    points_normed = (points - center) / max_dist\n",
    "    return points_normed\n",
    "\n",
    "\n",
    "def load_3d_data(file_path, num_points=10000, device=\"cuda\", do_normalize=True):\n",
    "    \"\"\"\n",
    "    Loads 3D data as PyTorch3D Pointclouds from either NPZ point cloud or OBJ mesh.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to either .npz point cloud or .obj mesh file\n",
    "        num_points: Number of points to sample if loading from mesh\n",
    "        device: Device to load data on\n",
    "\n",
    "    Returns:\n",
    "        Pointclouds object containing points and features\n",
    "    \"\"\"\n",
    "    file_ext = file_path.split('.')[-1].lower()\n",
    "\n",
    "    if file_ext == 'npz':\n",
    "        # Load NPZ point cloud directly like in the example\n",
    "        pointcloud = np.load(file_path)\n",
    "        verts = torch.Tensor(pointcloud['verts']).to(device)\n",
    "        rgb = torch.Tensor(pointcloud['rgb']).to(device)\n",
    "\n",
    "        print(\"lenght of the data\")\n",
    "        print(len(verts))\n",
    "\n",
    "        # Subsample if needed\n",
    "        if len(verts) > num_points:\n",
    "            idx = torch.randperm(len(verts))[:num_points]\n",
    "            verts = verts[idx]\n",
    "            rgb = rgb[idx]\n",
    "\n",
    "        if do_normalize:\n",
    "            verts = bounding_sphere_normalize(verts)\n",
    "\n",
    "        # Return both the points tensor and the Pointclouds object\n",
    "        point_cloud = Pointclouds(points=[verts], features=[rgb])\n",
    "        return verts, point_cloud  # Return both\n",
    "\n",
    "    elif file_ext == 'obj':\n",
    "        # Load and convert your OBJ file\n",
    "        points, point_cloud = obj_to_pointcloud(\n",
    "            file_path,\n",
    "            num_points=num_points,  # Adjust this number as needed\n",
    "            device=\"cuda\"  # Use \"cpu\" if you don't have a GPU\n",
    "        )\n",
    "        if do_normalize:\n",
    "            points = bounding_sphere_normalize(points)\n",
    "            # here we update the point cloud too\n",
    "            rgb = point_cloud.features_packed() # shape [N,3]\n",
    "            point_cloud = Pointclouds(points = [points], features = [rgb])\n",
    "        return points, point_cloud\n",
    "        # # Load mesh and sample points\n",
    "        # mesh = Mesh(file_path)\n",
    "        # vertices = mesh.vertices\n",
    "\n",
    "        # # Sample random points\n",
    "        # idx = torch.randperm(vertices.shape[0])[:num_points]\n",
    "        # points = vertices[idx].to(device)\n",
    "\n",
    "        # # Initialize with gray color\n",
    "        # colors = torch.ones_like(points) * 0.7\n",
    "\n",
    "        # return Pointclouds(points=[points], features=[colors])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_ext}. Only .npz and .obj are supported.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T14:25:41.677365Z",
     "iopub.status.busy": "2024-12-29T14:25:41.677027Z",
     "iopub.status.idle": "2024-12-29T14:25:41.682294Z",
     "shell.execute_reply": "2024-12-29T14:25:41.681359Z",
     "shell.execute_reply.started": "2024-12-29T14:25:41.677336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_grad_fn(tensor, depth=0):\n",
    "    \"\"\"Recursively print the gradient function graph\"\"\"\n",
    "    if tensor.grad_fn is None:\n",
    "        print(\"  \" * depth + \"None (leaf tensor)\")\n",
    "        return\n",
    "\n",
    "    print(\"  \" * depth + str(tensor.grad_fn))\n",
    "    for fn in tensor.grad_fn.next_functions:\n",
    "        if fn[0] is not None:\n",
    "            print(\"  \" * (depth + 1) + str(fn[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T14:37:38.441009Z",
     "iopub.status.busy": "2024-12-29T14:37:38.440674Z",
     "iopub.status.idle": "2024-12-29T14:37:38.456613Z",
     "shell.execute_reply": "2024-12-29T14:37:38.455726Z",
     "shell.execute_reply.started": "2024-12-29T14:37:38.440981Z"
    },
    "id": "E0SBrmlBkwib",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from src.render.cloud_point_renderer import MultiViewPointCloudRenderer\n",
    "from src.save_results import save_renders, save_results\n",
    "from src.neural_highlighter import NeuralHighlighter\n",
    "from src.Clip.loss_function import clip_loss\n",
    "from src.Clip.clip_model import get_clip_model, encode_text, setup_clip_transforms\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "# Set a consistent seed for reproducibility\n",
    "seed = 0  # You can use any integer value\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def optimize_point_cloud(points, clip_model, renderer, encoded_text, log_dir: str, **kwargs):\n",
    "    num_iterations = kwargs.get('num_iterations', 1000)\n",
    "    learning_rate = kwargs.get('learning_rate', 1e-4)\n",
    "    depth = kwargs.get('depth', 5)\n",
    "    width = kwargs.get('network_width', 256)\n",
    "    n_views = kwargs.get(\"n_views\", 4)\n",
    "    n_augs = kwargs.get('n_augs', 1)\n",
    "    clipavg = kwargs.get('clipavg', 'view')\n",
    "    device = kwargs.get('device', 'cuda')\n",
    "\n",
    "    # Initialize network and optimizer\n",
    "    net = NeuralHighlighter(\n",
    "        depth=depth,  # Number of hidden layers\n",
    "        width=width,  # Width of each layer\n",
    "        out_dim=2,  # Binary classification (highlight/no-highlight)\n",
    "        input_dim=3,  # 3D coordinates (x,y,z)\n",
    "        positional_encoding=False  # As recommended in the paper\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Set up the transforms\n",
    "    clip_transform, augment_transform = setup_clip_transforms()\n",
    "\n",
    "    # Training loop\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict highlight probabilities\n",
    "        pred_class = net(points)\n",
    "\n",
    "        # Create colors based on predictions\n",
    "        highlight_color = torch.tensor([204 / 255, 1.0, 0.0]).to(device)\n",
    "        base_color = torch.tensor([180 / 255, 180 / 255, 180 / 255]).to(device)\n",
    "\n",
    "        colors = pred_class[:, 0:1] * highlight_color + pred_class[:, 1:2] * base_color\n",
    "\n",
    "        # Create and render point cloud\n",
    "        point_cloud = renderer.create_point_cloud(points, colors)\n",
    "        rendered_images = renderer.render_all_views(point_cloud=point_cloud, n_views=n_views)\n",
    "        # Convert dictionary of images to tensor\n",
    "        rendered_tensor = []\n",
    "        for name, img in rendered_images.items():\n",
    "            rendered_tensor.append(img.to(device))\n",
    "        rendered_tensor = torch.stack(rendered_tensor)\n",
    "\n",
    "        #Convert rendered images to CLIP format\n",
    "        rendered_images = rendered_tensor.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
    "        #print(rendered_images.shape)\n",
    "\n",
    "        # Calculate CLIP loss\n",
    "        loss = clip_loss(\n",
    "            rendered_images=rendered_images,\n",
    "            encoded_text=encoded_text,\n",
    "            clip_transform=clip_transform,\n",
    "            augment_transform=augment_transform,\n",
    "            clip_model=clip_model,\n",
    "            n_augs=n_augs,\n",
    "            clipavg=clipavg\n",
    "        )\n",
    "        #print(\"Loss computation graph:\")\n",
    "        #print_grad_fn(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n",
    "            save_renders(log_dir, i, rendered_images)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def main(input_path, object_name, highlight_region, **kwargs):\n",
    "    \"\"\"\n",
    "    Main function for 3D highlighting with configurable parameters.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input 3D file (mesh or point cloud)\n",
    "        object_name: Name of the object for the prompt\n",
    "        highlight_region: Region to highlight\n",
    "        **kwargs: Optional parameters with defaults:\n",
    "            n_views: Number of views to render (default: 5)\n",
    "            n_aug: Number of augmentations (default: 5) \n",
    "            clipavg: Method for CLIP averaging (default: \"view\")\n",
    "            network_depth: Depth of neural network (default: 5)\n",
    "            network_width: Width of neural layers (default: 256)\n",
    "            learning_rate: Learning rate for optimization (default: 1e-4)\n",
    "            num_iterations: Number of training iterations (default: 500)\n",
    "            num_points: Number of points to sample (default: 10000)\n",
    "            device: Device to run on (default: \"cuda\")\n",
    "            output_dir: Directory for outputs (default: \"./output\")\n",
    "    \"\"\"\n",
    "    # Extract parameters from kwargs with defaults\n",
    "    n_views = kwargs.get(\"n_views\", 4)\n",
    "    num_points = kwargs.get(\"num_points\", 10000)\n",
    "    device = kwargs.get(\"device\", \"cuda\")\n",
    "    output_dir = kwargs.get(\"output_dir\", \"./output\")\n",
    "    do_normalize = kwargs.get(\"do_normalize\", True) \n",
    "    \n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Load 3D data (either mesh or point cloud)\n",
    "        print(f\"Loading 3D data from {input_path}...\")\n",
    "        points, point_cloud = load_3d_data(input_path, num_points=num_points, device=device)\n",
    "        print(f\"Loaded {len(points)} points\")\n",
    "\n",
    "        # Setup CLIP model\n",
    "        print(\"Setting up CLIP model...\")\n",
    "        clip_model, preprocess, resolution = get_clip_model()\n",
    "\n",
    "        # Create and encode prompt\n",
    "        prompt = f\"A 3D render of a gray {object_name} with highlighted {highlight_region}\"\n",
    "        print(f\"Using prompt: {prompt}\")\n",
    "        text_features = encode_text(clip_model, prompt, device)\n",
    "\n",
    "        # Initialize renderer\n",
    "        print(\"Setting up renderer...\")\n",
    "        renderer = MultiViewPointCloudRenderer(\n",
    "            image_size=512,\n",
    "            base_dist=30,  # Your default view distance\n",
    "            base_elev=10,  # Your default elevation\n",
    "            base_azim=0,  # Your default azimuth\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Optimize point cloud highlighting\n",
    "        print(\"Starting optimization...\")\n",
    "        net = optimize_point_cloud(\n",
    "            points=points,\n",
    "            renderer=renderer,\n",
    "            clip_model=clip_model,\n",
    "            encoded_text=text_features,\n",
    "            log_dir=output_dir,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Save results\n",
    "        print(\"Saving results...\")\n",
    "        save_results(\n",
    "            net=net,\n",
    "            points=points,\n",
    "            n_views=n_views,\n",
    "            prompt=prompt,\n",
    "            output_dir=output_dir,\n",
    "            renderer=renderer,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        print(\"Processing complete!\")\n",
    "        return net, points\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-29T14:51:45.103620Z",
     "iopub.status.busy": "2024-12-29T14:51:45.103314Z",
     "iopub.status.idle": "2024-12-29T14:55:01.180282Z",
     "shell.execute_reply": "2024-12-29T14:55:01.179493Z",
     "shell.execute_reply.started": "2024-12-29T14:51:45.103597Z"
    },
    "id": "uD_gYzdOpy-t",
    "outputId": "d0563f73-0957-4301-d62e-4937f39216e6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "main(\n",
    "    input_path=\"/kaggle/working/Affordance3DHighlighter/data/candle.obj\",\n",
    "    object_name=\"candle\",\n",
    "    highlight_region=\"head\",\n",
    "    n_views=4,\n",
    "    n_augs=1,\n",
    "    clipavg=\"view\",\n",
    "    network_depth=5,\n",
    "    network_width=256,\n",
    "    learning_rate=1e-4,\n",
    "    num_iterations=500,\n",
    "    num_points=100000,\n",
    "    device=\"cuda\",\n",
    "    output_dir=\"./output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for part 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main for the other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation_fullshape import evaluate_single_object, visualize_single_object\n",
    "from src.data_loader_fullshape import FullShapeDataset, create_dataset_splits\n",
    "from src.render.cloud_point_renderer import MultiViewPointCloudRenderer\n",
    "from src.neural_highlighter import NeuralHighlighter\n",
    "from src.Clip.clip_model import get_clip_model, encode_text\n",
    "\n",
    "def main(data_entry, net, clip_model, renderer, device=\"cuda\", **kwargs):\n",
    "    \"\"\"\n",
    "    Main function to process a single dataset entry.\n",
    "    Args:\n",
    "        data_entry (dict): Single object data from the dataset.\n",
    "        net: Neural highlighting model.\n",
    "        clip_model: CLIP model.\n",
    "        renderer: Renderer for visualization.\n",
    "        device (str): Device for computation.\n",
    "        **kwargs: Additional parameters for optimization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract information from the dataset entry\n",
    "        points = data_entry[\"coords\"]  # Nx3 point cloud\n",
    "        shape_id = data_entry[\"shape_id\"]\n",
    "        shape_class = data_entry[\"shape_class\"]\n",
    "        highlight_region = data_entry[\"affordances\"][0]  # Use the first affordance for testing\n",
    "        \n",
    "        # Generate prompt\n",
    "        prompt = f\"A 3D render of a gray {shape_class} with highlighted {highlight_region}\"\n",
    "        print(f\"Using prompt: {prompt}\")\n",
    "        text_features = encode_text(clip_model, prompt, device)\n",
    "\n",
    "        # Optimize point cloud highlighting\n",
    "        print(\"Starting optimization...\")\n",
    "        net = optimize_point_cloud(\n",
    "            points=points,\n",
    "            renderer=renderer,\n",
    "            clip_model=clip_model,\n",
    "            encoded_text=text_features,\n",
    "            log_dir=kwargs.get(\"output_dir\", \"./output\"),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Save results\n",
    "        print(\"Saving results...\")\n",
    "        save_results(\n",
    "            net=net,\n",
    "            points=points,\n",
    "            n_views=kwargs.get(\"n_views\", 4),\n",
    "            prompt=prompt,\n",
    "            output_dir=kwargs.get(\"output_dir\", \"./output\"),\n",
    "            renderer=renderer,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        print(f\"Processing complete for shape_id: {shape_id}\")\n",
    "        \n",
    "        # Optional visualization\n",
    "        if kwargs.get(\"visualize\", True):\n",
    "            visualize_single_object(data_entry, net, clip_model, device=device, out_dir=kwargs.get(\"output_dir\", \"./output\"))\n",
    "\n",
    "        return net, points\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing shape_id {data_entry['shape_id']}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Loading the dataset\n",
    "# We only use the val_data and test_data for part 3. 10 percent of train set is validation set \n",
    "# and 5 percent of train set is test set.\n",
    "# also when loading the dataset (better seen in Dataset Loader), specific classes and affordance labels \n",
    "# have been filtered as per the req in part 3. \n",
    "dataset = FullShapeDataset(\"/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl\", device=\"cuda\")\n",
    "train_data, val_data, test_data = create_dataset_splits(dataset, val_ratio=0.1, test_ratio=0.05)\n",
    "\n",
    "# Ensure test set is not empty\n",
    "if len(test_data) == 0:\n",
    "    raise ValueError(\"Test dataset is empty. Check your dataset and split ratios.\")\n",
    "\n",
    "# Select a single object from the test set\n",
    "data_index = 0  # You can adjust this to test different objects\n",
    "data_entry = test_data[data_index]\n",
    "\n",
    "# Setup CLIP and Renderer\n",
    "clip_model, preprocess, resolution = get_clip_model()\n",
    "renderer = MultiViewPointCloudRenderer(image_size=512, base_dist=30, base_elev=10, device=\"cuda\")\n",
    "\n",
    "# Setup Neural Highlighter\n",
    "net = NeuralHighlighter(depth=5, width=256, out_dim=2, input_dim=3).to(\"cuda\")\n",
    "\n",
    "# Run the main function for a single object\n",
    "main(\n",
    "    data_entry=data_entry,\n",
    "    net=net,\n",
    "    clip_model=clip_model,\n",
    "    renderer=renderer,\n",
    "    device=\"cuda\",\n",
    "    num_iterations=500,\n",
    "    learning_rate=1e-4,\n",
    "    output_dir=\"./results\",\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# Evaluate affordances for the object\n",
    "results = evaluate_single_object(data_entry, net, clip_model, device=\"cuda\")\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_single_object(data_entry, net, clip_model, device=\"cuda\", out_dir=\"./results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New strategy for evaluation\n",
    "Hyperparam + Strategy Tuning, Then Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation_fullshape import (\n",
    "    compute_mIoU,  # needed for final checks\n",
    "    evaluate_single_object,\n",
    "    grid_search_validation,   # We'll define a simple approach\n",
    "    evaluate_dataset\n",
    ")\n",
    "\n",
    "# Here we define a short list of strategies and thresholds. \n",
    "strategies_list = [\"basic\", \"functional\", \"descriptive\", \"action\"]\n",
    "thresholds_list = [0.3, 0.5, 0.7]\n",
    "\n",
    "clip_model2, _, _= get_clip_model()\n",
    "renderer2 = MultiViewPointCloudRenderer(\n",
    "    image_size=512, base_dist=30, base_elev=10, device=\"cuda\"\n",
    ")\n",
    "net2 = NeuralHighlighter(depth=5, width=256, out_dim=2, input_dim=3).to(\"cuda\")\n",
    "\n",
    "# Here we pick 4 objects from val\n",
    "val_size = len(val_data)\n",
    "num_val_objects = min(4, val_size)\n",
    "val_indices = list(range(num_val_objects))\n",
    "\n",
    "best_strat = None\n",
    "best_th = None \n",
    "best_iou = 1.0 \n",
    "\n",
    "# For each of these 4 val objects, we train from scratch. Then measure how well each (strat, threshold)\n",
    "# for now, perform across all affordances of that shape. \n",
    "# Then we will average the IoU across these 4 shapes to pick the best approach. \n",
    "val_results = [] # Here we store shape level IoU so we can compute the average. \n",
    "\n",
    "for strategy in strategies_list:\n",
    "    for threshold in thresholds_list:\n",
    "        # here we will accumulate the iou across the 4 shapes\n",
    "        sum_iou = 0.0\n",
    "        count = 0\n",
    "        for idx in val_indices:\n",
    "            val_entry = val_data[idx]\n",
    "            # now we train the network for the shape\n",
    "            shape_net = NeuralHighlighter(depth = 5, width = 256, out_dim = 2, input_dim=3).cuda()\n",
    "            shape_coords = val_entry[\"coords\"]\n",
    "            shape_class = val_entry[\"shape_class\"]\n",
    "            # Here we pick the first affordance for the main prompt\n",
    "            aff = val_entry[\"affordances\"][0]\n",
    "            prompt = f\"A 3D render of a gray {shape_class} with highlighted {aff}\"\n",
    "\n",
    "            # short training \n",
    "            txt_feats = encode_text(clip_model2, prompt, device = \"cuda\")\n",
    "            shape_renderer = MultiViewPointCloudRenderer(image_size = 256, base_dist=20, base_elev=10, device=\"cuda\")\n",
    "            shape_net=optimize_point_cloud(\n",
    "                points=shape_coords,\n",
    "                clip_model=clip_model2,\n",
    "                renderer=shape_renderer,\n",
    "                encoded_text=txt_feats,\n",
    "                log_dir=\"./val_tmp\",\n",
    "                num_iterations=200,\n",
    "                device=\"cuda\",\n",
    "                n_views=2\n",
    "            )\n",
    "\n",
    "            # measure IoU across all affs in that shape with (strategy, threshold)\n",
    "            aff_list = val_entry[\"affordances\"]\n",
    "            shape_sum = 0.0\n",
    "            c2 = 0\n",
    "            with torch.no_grad():\n",
    "                pred2 = shape_net(shape_coords)\n",
    "                highlight_prob2 = pred2[:, 0]\n",
    "            for a2 in aff_list:\n",
    "                gt_bin=(val_entry[\"labels_dict\"][a2]>0.5).long()\n",
    "                # apply threshold\n",
    "                bin_pred=(highlight_prob2>=threshold).long()\n",
    "                iou_val=compute_mIoU(bin_pred, gt_bin)\n",
    "                shape_sum+=iou_val\n",
    "                c2+=1\n",
    "\n",
    "            shape_mean=shape_sum/c2 if c2>0 else 0.0\n",
    "            sum_iou+=shape_mean\n",
    "            count+=1\n",
    "            \n",
    "        avg_iou= sum_iou/count if count>0 else 0.0\n",
    "        val_results.append((strategy,threshold,avg_iou))\n",
    "        if avg_iou>best_iou:\n",
    "            best_iou=avg_iou\n",
    "            best_strat=strategy\n",
    "            best_th=threshold\n",
    "\n",
    "print(f\"[Val Done] best strategy={best_strat}, threshold={best_th}, meanIoU={best_iou:.3f}\")\n",
    "\n",
    "\n",
    "# Now with the best hyperparameters , strategy and threshold achieved we check the test set. \n",
    "# Now test with best\n",
    "test_count= len(test_data)\n",
    "test_iou_sum=0.0\n",
    "for tidx in range(test_count):\n",
    "    test_entry=test_data[tidx]\n",
    "    # train a net for that shape\n",
    "    shape_net2=NeuralHighlighter(depth=5, width=256, out_dim=2, input_dim=3).cuda()\n",
    "    shape_coords2=test_entry[\"coords\"]\n",
    "    aff_main2=test_entry[\"affordances\"][0]\n",
    "    prompt_main2=f\"A 3D render of a gray {test_entry['shape_class']} with highlighted {aff_main2}\"\n",
    "\n",
    "    txt_feats_main2=encode_text(clip_model2, prompt_main2, device=\"cuda\")\n",
    "    test_renderer=MultiViewPointCloudRenderer(image_size=256, base_dist=20, base_elev=10, device=\"cuda\")\n",
    "\n",
    "    shape_net2=optimize_point_cloud(\n",
    "        points=shape_coords2,\n",
    "        clip_model=clip_model2,\n",
    "        renderer=test_renderer,\n",
    "        encoded_text=txt_feats_main2,\n",
    "        log_dir=\"./test_tmp\",\n",
    "        num_iterations=200,\n",
    "        device=\"cuda\",\n",
    "        n_views=2\n",
    "    )\n",
    "\n",
    "    # measure iou across all affs with best_strat, best_th\n",
    "    sum_tiou=0.0\n",
    "    c3=0\n",
    "    with torch.no_grad():\n",
    "        pclass_test=shape_net2(shape_coords2)\n",
    "        highlight_prob_test= pclass_test[:,0]\n",
    "    for aff_tst in test_entry[\"affordances\"]:\n",
    "        gt_tst=(test_entry[\"labels_dict\"][aff_tst]>0.5).long()\n",
    "        bin_preds_tst=(highlight_prob_test>=best_th).long()\n",
    "        iou_test=compute_mIoU(bin_preds_tst, gt_tst)\n",
    "        sum_tiou+=iou_test\n",
    "        c3+=1\n",
    "    shape_avg_tiou= sum_tiou/c3 if c3>0 else 0.0\n",
    "    test_iou_sum+=shape_avg_tiou\n",
    "\n",
    "final_test_mIoU = test_iou_sum/test_count if test_count>0 else 0.0\n",
    "print(f\"[Test] Using best strategy={best_strat}, threshold={best_th}, final test mIoU={final_test_mIoU:.3f}\")\n",
    "\n",
    "# We can also visualize one test shape's multi-view using the highlight probability\n",
    "print(\"\\nVisualizing multi-view for the last test shape with final threshold:\")\n",
    "test_shape = test_data[test_count-1]\n",
    "coords_test = test_shape[\"coords\"]\n",
    "with torch.no_grad():\n",
    "    test_pred = shape_net2(coords_test)\n",
    "    highlight_sc = test_pred[:,0]\n",
    "\n",
    "# let's do a quick multi-view\n",
    "test_cloud = test_renderer.create_point_cloud(\n",
    "    coords_test,\n",
    "    highlight_sc.unsqueeze(1)*torch.tensor([204/255,1.0,0.0],device=\"cuda\") +\n",
    "    (1.0 - highlight_sc.unsqueeze(1))*torch.tensor([180/255,180/255,180/255],device=\"cuda\")\n",
    ")\n",
    "rendered_testviews = test_renderer.render_all_views(test_cloud,n_views=4)\n",
    "import matplotlib.pyplot as plt\n",
    "fig,axes=plt.subplots(1,len(rendered_testviews),figsize=(4*len(rendered_testviews),4))\n",
    "for ax,(vname,imgT) in zip(axes, rendered_testviews.items()):\n",
    "    ax.imshow(imgT.cpu().numpy())\n",
    "    ax.set_title(vname)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(f\"Test Shape {test_shape['shape_id']} - final threshold={best_th}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
