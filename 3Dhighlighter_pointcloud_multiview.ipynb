{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-01-01T14:32:38.498175Z",
     "iopub.execute_input": "2025-01-01T14:32:38.498457Z",
     "iopub.status.idle": "2025-01-01T14:32:39.660922Z",
     "shell.execute_reply.started": "2025-01-01T14:32:38.498434Z",
     "shell.execute_reply": "2025-01-01T14:32:39.660009Z"
    },
    "id": "2rqJobuCpQLi",
    "outputId": "281b69ef-7d5c-40e7-e9d1-c343706616ea",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Cloning into 'Affordance3DHighlighter'...\nremote: Enumerating objects: 233, done.\u001B[K\nremote: Counting objects: 100% (233/233), done.\u001B[K\nremote: Compressing objects: 100% (161/161), done.\u001B[K\nremote: Total 233 (delta 137), reused 158 (delta 69), pack-reused 0 (from 0)\u001B[K\nReceiving objects: 100% (233/233), 2.49 MiB | 16.78 MiB/s, done.\nResolving deltas: 100% (137/137), done.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import os\n\nos.chdir('/kaggle/working/Affordance3DHighlighter')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-01T14:32:43.089369Z",
     "iopub.execute_input": "2025-01-01T14:32:43.089724Z",
     "iopub.status.idle": "2025-01-01T14:32:43.093468Z",
     "shell.execute_reply.started": "2025-01-01T14:32:43.089694Z",
     "shell.execute_reply": "2025-01-01T14:32:43.092660Z"
    },
    "id": "fhLYzi952EEA",
    "trusted": true
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "!pip install gdown\n!gdown --id 1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\n!unzip full-shape.zip -d /kaggle/working/Affordance3DHighlighter/data/",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-01T14:32:45.542095Z",
     "iopub.execute_input": "2025-01-01T14:32:45.542385Z",
     "iopub.status.idle": "2025-01-01T14:33:16.869764Z",
     "shell.execute_reply.started": "2025-01-01T14:32:45.542363Z",
     "shell.execute_reply": "2025-01-01T14:33:16.868596Z"
    },
    "trusted": true,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\nFrom (redirected): https://drive.google.com/uc?id=1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF&confirm=t&uuid=7850611d-9279-4785-829f-e1d2eae22256\nTo: /kaggle/working/Affordance3DHighlighter/full-shape.zip\n100%|█████████████████████████████████████████| 558M/558M [00:03<00:00, 185MB/s]\nArchive:  full-shape.zip\n  inflating: /kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl  \n  inflating: /kaggle/working/Affordance3DHighlighter/data/full_shape_val_data.pkl  \n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "import pickle\n\n# Load training data\nwith open('/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl', 'rb') as train_file:\n    train_data = pickle.load(train_file)\n# Inspect the contents\nprint(f\"Training Data Type: {type(train_data)}\")\nprint(f\"Training Data Example: {train_data[:1]}\")",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-31T00:37:15.380198Z",
     "iopub.status.busy": "2024-12-31T00:37:15.379956Z",
     "iopub.status.idle": "2024-12-31T00:37:19.828311Z",
     "shell.execute_reply": "2024-12-31T00:37:19.827376Z",
     "shell.execute_reply.started": "2024-12-31T00:37:15.380179Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install git+https://github.com/openai/CLIP.git\n!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-01-01T14:35:07.250511Z",
     "iopub.execute_input": "2025-01-01T14:35:07.250838Z",
     "iopub.status.idle": "2025-01-01T14:35:23.271738Z",
     "shell.execute_reply.started": "2025-01-01T14:35:07.250813Z",
     "shell.execute_reply": "2025-01-01T14:35:23.270687Z"
    },
    "id": "d8vCbctxbPP4",
    "outputId": "55a100ed-7b85-4630-9e3f-e3ce98c37fff",
    "trusted": true,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-bfzmhcp3\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-bfzmhcp3\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.5)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.19.1+cu121)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.6.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (10.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.8/44.8 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001B[?25l\u001B[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=d0d1d5f6c874aaef1b66bb3a077d6b7674db1bbd1ee4caf4c0b17a777b7a64a7\n  Stored in directory: /tmp/pip-ephem-wheel-cache-phc7d0nx/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\nLooking in links: https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\nCollecting kaolin==0.17.0\n  Downloading https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121/kaolin-0.17.0-cp310-cp310-linux_x86_64.whl (5.4 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.4/5.4 MB\u001B[0m \u001B[31m34.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting ipycanvas (from kaolin==0.17.0)\n  Downloading ipycanvas-0.13.3-py2.py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: ipyevents in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (2.0.2)\nCollecting jupyter-client<8 (from kaolin==0.17.0)\n  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (2.2.5)\nRequirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (6.3.3)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (0.2.2)\nCollecting usd-core (from kaolin==0.17.0)\n  Downloading usd_core-24.11-cp310-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.26.4)\nRequirement already satisfied: pybind11 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (2.13.6)\nRequirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (10.4.0)\nRequirement already satisfied: tqdm>=4.51.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (4.66.5)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.13.1)\nRequirement already satisfied: pygltflib in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.16.3)\nCollecting warp-lang (from kaolin==0.17.0)\n  Downloading warp_lang-1.5.0-py3-none-manylinux2014_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (7.34.0)\nRequirement already satisfied: traitlets>=4 in /usr/local/lib/python3.10/dist-packages (from comm>=0.1.3->kaolin==0.17.0) (5.7.1)\nRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (0.4)\nRequirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (5.7.2)\nRequirement already satisfied: nest-asyncio>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (1.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (2.8.2)\nRequirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (24.0.1)\nRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (3.0.4)\nRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (2.2.0)\nRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (8.1.7)\nRequirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from ipycanvas->kaolin==0.17.0) (8.1.5)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (71.0.4)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (3.0.47)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (2.18.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (4.9.0)\nRequirement already satisfied: dataclasses-json>=0.0.25 in /usr/local/lib/python3.10/dist-packages (from pygltflib->kaolin==0.17.0) (0.6.7)\nRequirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from pygltflib->kaolin==0.17.0) (1.2.15)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (3.23.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (0.9.0)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.0.13)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->kaolin==0.17.0) (0.8.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->kaolin==0.17.0) (2.1.5)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.9.2->jupyter-client<8->kaolin==0.17.0) (4.3.6)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->kaolin==0.17.0) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kaolin==0.17.0) (0.2.13)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->jupyter-client<8->kaolin==0.17.0) (1.16.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pygltflib->kaolin==0.17.0) (1.16.0)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (24.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (1.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (4.12.2)\nDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.5/133.5 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading ipycanvas-0.13.3-py2.py3-none-any.whl (125 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.8/125.8 kB\u001B[0m \u001B[31m8.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading usd_core-24.11-cp310-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.4 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m25.4/25.4 MB\u001B[0m \u001B[31m77.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading warp_lang-1.5.0-py3-none-manylinux2014_x86_64.whl (84.6 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.6/84.6 MB\u001B[0m \u001B[31m20.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: warp-lang, usd-core, jupyter-client, ipycanvas, kaolin\n  Attempting uninstall: jupyter-client\n    Found existing installation: jupyter_client 8.6.3\n    Uninstalling jupyter_client-8.6.3:\n      Successfully uninstalled jupyter_client-8.6.3\nSuccessfully installed ipycanvas-0.13.3 jupyter-client-7.4.9 kaolin-0.17.0 usd-core-24.11 warp-lang-1.5.0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "\nimport sys\nimport torch\n\nneed_pytorch3d = False\ntry:\n    import pytorch3d\nexcept ModuleNotFoundError:\n    need_pytorch3d = True\nif need_pytorch3d:\n    pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n    version_str = \"\".join([\n        f\"py3{sys.version_info.minor}_cu\",\n        torch.version.cuda.replace(\".\", \"\"),\n        f\"_pyt{pyt_version_str}\"\n    ])\n    !pip install iopath\n    if sys.platform.startswith(\"linux\"):\n        print(\"Trying to install wheel for PyTorch3D\")\n        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n        pip_list = !pip freeze\n        need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for i in pip_list)\n    if need_pytorch3d:\n        print(f\"failed to find/install wheel for {version_str}\")\nif need_pytorch3d:\n    print(\"Installing PyTorch3D from source\")\n    !pip install ninja\n    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T14:58:53.585274Z",
     "start_time": "2024-12-26T14:58:48.836644Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-01-01T14:36:21.555574Z",
     "iopub.execute_input": "2025-01-01T14:36:21.555908Z",
     "iopub.status.idle": "2025-01-01T14:36:21.568059Z",
     "shell.execute_reply.started": "2025-01-01T14:36:21.555881Z",
     "shell.execute_reply": "2025-01-01T14:36:21.567172Z"
    },
    "id": "sRNWfRMnIzuJ",
    "outputId": "39ed1f77-3f8e-499d-eb2b-8aecadf7c335",
    "trusted": true
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "!pip install open3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-01-01T14:36:14.047506Z",
     "iopub.execute_input": "2025-01-01T14:36:14.047869Z",
     "iopub.status.idle": "2025-01-01T14:36:17.370993Z",
     "shell.execute_reply.started": "2025-01-01T14:36:14.047843Z",
     "shell.execute_reply": "2025-01-01T14:36:17.369917Z"
    },
    "id": "zT9LfpcRXDHy",
    "outputId": "527d847c-60da-4e72-c096-81cbf9671f92",
    "trusted": true,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: open3d in /usr/local/lib/python3.10/dist-packages (0.18.0)\nRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.26.4)\nRequirement already satisfied: dash>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.18.2)\nRequirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.4)\nRequirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\nRequirement already satisfied: configargparse in /usr/local/lib/python3.10/dist-packages (from open3d) (1.7)\nRequirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from open3d) (8.1.5)\nRequirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from open3d) (2.4.0)\nRequirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (10.4.0)\nRequirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.7.1)\nRequirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.1.4)\nRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.2)\nRequirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.66.5)\nRequirement already satisfied: pyquaternion in /usr/local/lib/python3.10/dist-packages (from open3d) (0.9.9)\nRequirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.2.5)\nRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\nRequirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\nRequirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\nRequirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.0.0)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.5.0)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\nRequirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.3.4)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (71.0.4)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (24.1)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\nRequirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.20.0)\nRequirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (2.1.5)\nRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\nRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.47)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.20.0)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.16.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.20.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.8.30)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "!mkdir -p data/PittsburghBridge\n!wget -P data/PittsburghBridge https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-30T21:49:30.907987Z",
     "iopub.status.busy": "2024-12-30T21:49:30.907664Z",
     "iopub.status.idle": "2024-12-30T21:49:31.604721Z",
     "shell.execute_reply": "2024-12-30T21:49:31.603879Z",
     "shell.execute_reply.started": "2024-12-30T21:49:30.907957Z"
    },
    "id": "FTk44DGYYIBS",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "864b8bfd-a638-4f09-f520-d3f0fb75389c",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\nfrom src.mesh import Mesh\nfrom pytorch3d.structures import Pointclouds\n\nfrom src.convertor import obj_to_pointcloud\n\n\ndef bounding_sphere_normalize(points: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    points: (N,3) tensor of point coords\n    Return normalized points in a unit sphere centered at origin.\n    \"\"\"\n    center = points.mean(dim=0, keepdim=True)\n    max_dist = (points - center).norm(p=2, dim=1).max()\n    points_normed = (points - center) / max_dist\n    return points_normed\n\n\ndef load_3d_data(file_path, num_points=10000, device=\"cuda\", do_normalize=True):\n    \"\"\"\n    Loads 3D data as PyTorch3D Pointclouds from either NPZ point cloud or OBJ mesh.\n\n    Args:\n        file_path: Path to either .npz point cloud or .obj mesh file\n        num_points: Number of points to sample if loading from mesh\n        device: Device to load data on\n\n    Returns:\n        Pointclouds object containing points and features\n    \"\"\"\n    file_ext = file_path.split('.')[-1].lower()\n\n    if file_ext == 'npz':\n        # Load NPZ point cloud directly like in the example\n        pointcloud = np.load(file_path)\n        verts = torch.Tensor(pointcloud['verts']).to(device)\n        rgb = torch.Tensor(pointcloud['rgb']).to(device)\n\n        print(\"lenght of the data\")\n        print(len(verts))\n\n        # Subsample if needed\n        if len(verts) > num_points:\n            idx = torch.randperm(len(verts))[:num_points]\n            verts = verts[idx]\n            rgb = rgb[idx]\n\n        if do_normalize:\n            verts = bounding_sphere_normalize(verts)\n\n        # Return both the points tensor and the Pointclouds object\n        point_cloud = Pointclouds(points=[verts], features=[rgb])\n        return verts, point_cloud  # Return both\n\n    elif file_ext == 'obj':\n        # Load and convert your OBJ file\n        points, point_cloud = obj_to_pointcloud(\n            file_path,\n            num_points=num_points,  # Adjust this number as needed\n            device=\"cuda\"  # Use \"cpu\" if you don't have a GPU\n        )\n        if do_normalize:\n            points = bounding_sphere_normalize(points)\n            # here we update the point cloud too\n            rgb = point_cloud.features_packed() # shape [N,3]\n            point_cloud = Pointclouds(points = [points], features = [rgb])\n        return points, point_cloud\n        # # Load mesh and sample points\n        # mesh = Mesh(file_path)\n        # vertices = mesh.vertices\n\n        # # Sample random points\n        # idx = torch.randperm(vertices.shape[0])[:num_points]\n        # points = vertices[idx].to(device)\n\n        # # Initialize with gray color\n        # colors = torch.ones_like(points) * 0.7\n\n        # return Pointclouds(points=[points], features=[colors])\n\n    else:\n        raise ValueError(f\"Unsupported file format: {file_ext}. Only .npz and .obj are supported.\")\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-01T14:36:26.236843Z",
     "iopub.execute_input": "2025-01-01T14:36:26.237129Z",
     "iopub.status.idle": "2025-01-01T14:36:29.035862Z",
     "shell.execute_reply.started": "2025-01-01T14:36:26.237107Z",
     "shell.execute_reply": "2025-01-01T14:36:29.035082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Warp 1.5.0 initialized:\n   CUDA Toolkit 12.6, Driver 12.6\n   Devices:\n     \"cpu\"      : \"x86_64\"\n     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n     \"cuda:1\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n   CUDA peer access:\n     Not supported\n   Kernel cache:\n     /root/.cache/warp/1.5.0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "def print_grad_fn(tensor, depth=0):\n    \"\"\"Recursively print the gradient function graph\"\"\"\n    if tensor.grad_fn is None:\n        print(\"  \" * depth + \"None (leaf tensor)\")\n        return\n\n    print(\"  \" * depth + str(tensor.grad_fn))\n    for fn in tensor.grad_fn.next_functions:\n        if fn[0] is not None:\n            print(\"  \" * (depth + 1) + str(fn[0]))",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-29T14:25:41.677365Z",
     "iopub.status.busy": "2024-12-29T14:25:41.677027Z",
     "iopub.status.idle": "2024-12-29T14:25:41.682294Z",
     "shell.execute_reply": "2024-12-29T14:25:41.681359Z",
     "shell.execute_reply.started": "2024-12-29T14:25:41.677336Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": "\nfrom src.render.cloud_point_renderer import MultiViewPointCloudRenderer\nfrom src.save_results import save_renders, save_results\nfrom src.neural_highlighter import NeuralHighlighter\nfrom src.Clip.loss_function import clip_loss\nfrom src.Clip.clip_model import get_clip_model, encode_text, setup_clip_transforms\n\nimport torch\nimport numpy as np\nimport random\nfrom tqdm import tqdm\n\n# Constrain most sources of randomness\n# (some torch backwards functions within CLIP are non-determinstic)\n# Set a consistent seed for reproducibility\nseed = 0  # You can use any integer value\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\n\ndef optimize_point_cloud(points, clip_model, renderer, encoded_text, log_dir: str, **kwargs):\n    num_iterations = kwargs.get('num_iterations', 1000)\n    learning_rate = kwargs.get('learning_rate', 1e-4)\n    depth = kwargs.get('depth', 5)\n    width = kwargs.get('network_width', 256)\n    n_views = kwargs.get(\"n_views\", 4)\n    n_augs = kwargs.get('n_augs', 1)\n    clipavg = kwargs.get('clipavg', 'view')\n    device = kwargs.get('device', 'cuda')\n\n    # Initialize network and optimizer\n    net = NeuralHighlighter(\n        depth=depth,  # Number of hidden layers\n        width=width,  # Width of each layer\n        out_dim=2,  # Binary classification (highlight/no-highlight)\n        input_dim=3,  # 3D coordinates (x,y,z)\n        positional_encoding=False  # As recommended in the paper\n    ).to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\n    # Set up the transforms\n    clip_transform, augment_transform = setup_clip_transforms()\n\n    # Training loop\n    for i in tqdm(range(num_iterations)):\n        optimizer.zero_grad()\n\n        # Predict highlight probabilities\n        pred_class = net(points)\n\n        # Create colors based on predictions\n        highlight_color = torch.tensor([204 / 255, 1.0, 0.0]).to(device)\n        base_color = torch.tensor([180 / 255, 180 / 255, 180 / 255]).to(device)\n\n        colors = pred_class[:, 0:1] * highlight_color + pred_class[:, 1:2] * base_color\n\n        # Create and render point cloud\n        point_cloud = renderer.create_point_cloud(points, colors)\n        rendered_images = renderer.render_all_views(point_cloud=point_cloud, n_views=n_views)\n        # Convert dictionary of images to tensor\n        rendered_tensor = []\n        for name, img in rendered_images.items():\n            rendered_tensor.append(img.to(device))\n        rendered_tensor = torch.stack(rendered_tensor)\n\n        #Convert rendered images to CLIP format\n        rendered_images = rendered_tensor.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n        #print(rendered_images.shape)\n\n        # Calculate CLIP loss\n        loss = clip_loss(\n            rendered_images=rendered_images,\n            encoded_text=encoded_text,\n            clip_transform=clip_transform,\n            augment_transform=augment_transform,\n            clip_model=clip_model,\n            n_augs=n_augs,\n            clipavg=clipavg\n        )\n        #print(\"Loss computation graph:\")\n        #print_grad_fn(loss)\n        loss.backward()\n        optimizer.step()\n\n        if i % 100 == 0:\n            print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n            save_renders(log_dir, i, rendered_images)\n\n    return net\n\n\ndef main(input_path, object_name, highlight_region, **kwargs):\n    \"\"\"\n    Main function for 3D highlighting with configurable parameters.\n    \n    Args:\n        input_path: Path to input 3D file (mesh or point cloud)\n        object_name: Name of the object for the prompt\n        highlight_region: Region to highlight\n        **kwargs: Optional parameters with defaults:\n            n_views: Number of views to render (default: 5)\n            n_aug: Number of augmentations (default: 5) \n            clipavg: Method for CLIP averaging (default: \"view\")\n            network_depth: Depth of neural network (default: 5)\n            network_width: Width of neural layers (default: 256)\n            learning_rate: Learning rate for optimization (default: 1e-4)\n            num_iterations: Number of training iterations (default: 500)\n            num_points: Number of points to sample (default: 10000)\n            device: Device to run on (default: \"cuda\")\n            output_dir: Directory for outputs (default: \"./output\")\n    \"\"\"\n    # Extract parameters from kwargs with defaults\n    n_views = kwargs.get(\"n_views\", 4)\n    num_points = kwargs.get(\"num_points\", 10000)\n    device = kwargs.get(\"device\", \"cuda\")\n    output_dir = kwargs.get(\"output_dir\", \"./output\")\n    do_normalize = kwargs.get(\"do_normalize\", True) \n    \n    try:\n        # Create output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Load 3D data (either mesh or point cloud)\n        print(f\"Loading 3D data from {input_path}...\")\n        points, point_cloud = load_3d_data(input_path, num_points=num_points, device=device)\n        print(f\"Loaded {len(points)} points\")\n\n        # Setup CLIP model\n        print(\"Setting up CLIP model...\")\n        clip_model, preprocess, resolution = get_clip_model()\n\n        # Create and encode prompt\n        prompt = f\"A 3D render of a gray {object_name} with highlighted {highlight_region}\"\n        print(f\"Using prompt: {prompt}\")\n        text_features = encode_text(clip_model, prompt, device)\n\n        # Initialize renderer\n        print(\"Setting up renderer...\")\n        renderer = MultiViewPointCloudRenderer(\n            image_size=512,\n            base_dist=30,  # Your default view distance\n            base_elev=10,  # Your default elevation\n            base_azim=0,  # Your default azimuth\n            device=device\n        )\n\n        # Optimize point cloud highlighting\n        print(\"Starting optimization...\")\n        net = optimize_point_cloud(\n            points=points,\n            renderer=renderer,\n            clip_model=clip_model,\n            encoded_text=text_features,\n            log_dir=output_dir,\n            **kwargs\n        )\n\n        # Save results\n        print(\"Saving results...\")\n        save_results(\n            net=net,\n            points=points,\n            n_views=n_views,\n            prompt=prompt,\n            output_dir=output_dir,\n            renderer=renderer,\n            device=device\n        )\n\n        print(\"Processing complete!\")\n        return net, points\n\n    except Exception as e:\n        print(f\"Error in processing: {str(e)}\")\n        raise\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-01T14:36:34.620779Z",
     "iopub.execute_input": "2025-01-01T14:36:34.621176Z",
     "iopub.status.idle": "2025-01-01T14:36:36.982381Z",
     "shell.execute_reply.started": "2025-01-01T14:36:34.621151Z",
     "shell.execute_reply": "2025-01-01T14:36:36.981685Z"
    },
    "id": "E0SBrmlBkwib",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Jupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "main(\n    input_path=\"/kaggle/working/Affordance3DHighlighter/data/candle.obj\",\n    object_name=\"candle\",\n    highlight_region=\"head\",\n    n_views=4,\n    n_augs=1,\n    clipavg=\"view\",\n    network_depth=5,\n    network_width=256,\n    learning_rate=1e-4,\n    num_iterations=500,\n    num_points=100000,\n    device=\"cuda\",\n    output_dir=\"./output\"\n)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-29T14:51:45.103620Z",
     "iopub.status.busy": "2024-12-29T14:51:45.103314Z",
     "iopub.status.idle": "2024-12-29T14:55:01.180282Z",
     "shell.execute_reply": "2024-12-29T14:55:01.179493Z",
     "shell.execute_reply.started": "2024-12-29T14:51:45.103597Z"
    },
    "id": "uD_gYzdOpy-t",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d0563f73-0957-4301-d62e-4937f39216e6",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Evaluation for part 3 ",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "This is just for visual representation of how the dataset objects look like",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from src.evaluation_fullshapev2 import evaluate_single_object, visualize_single_object\nfrom src.data_loader_fullshape import FullShapeDataset, create_dataset_splits\nfrom src.render.cloud_point_renderer import MultiViewPointCloudRenderer\nfrom src.neural_highlighter import NeuralHighlighter\nfrom src.Clip.clip_model import get_clip_model, encode_text\n\ndef main(data_entry, net, clip_model, renderer, device=\"cuda\", **kwargs):\n    \"\"\"\n    Main function to process a single dataset entry.\n    Args:\n        data_entry (dict): Single object data from the dataset.\n        net: Neural highlighting model.\n        clip_model: CLIP model.\n        renderer: Renderer for visualization.\n        device (str): Device for computation.\n        **kwargs: Additional parameters for optimization.\n    \"\"\"\n    try:\n        # Extract information from the dataset entry\n        points = data_entry[\"coords\"]  # Nx3 point cloud\n        shape_id = data_entry[\"shape_id\"]\n        shape_class = data_entry[\"shape_class\"]\n        highlight_region = data_entry[\"affordances\"][0]  # Use the first affordance for testing\n        \n        # Generate prompt\n        prompt = f\"A 3D render of a gray {shape_class} with highlighted {highlight_region}\"\n        print(f\"Using prompt: {prompt}\")\n        text_features = encode_text(clip_model, prompt, device)\n\n        # Optimize point cloud highlighting\n        print(\"Starting optimization...\")\n        net = optimize_point_cloud(\n            points=points,\n            renderer=renderer,\n            clip_model=clip_model,\n            encoded_text=text_features,\n            log_dir=kwargs.get(\"output_dir\", \"./output\"),\n            **kwargs\n        )\n\n        # Save results\n        print(\"Saving results...\")\n        save_results(\n            net=net,\n            points=points,\n            n_views=kwargs.get(\"n_views\", 4),\n            prompt=prompt,\n            output_dir=kwargs.get(\"output_dir\", \"./output\"),\n            renderer=renderer,\n            device=device\n        )\n\n        print(f\"Processing complete for shape_id: {shape_id}\")\n        \n        # Optional visualization\n        if kwargs.get(\"visualize\", True):\n            visualize_single_object(data_entry, net, clip_model, device=device, out_dir=kwargs.get(\"output_dir\", \"./output\"))\n\n        return net, points\n\n    except Exception as e:\n        print(f\"Error in processing shape_id {data_entry['shape_id']}: {str(e)}\")\n        raise\n\n# Loading the dataset\n# We only use the val_data and test_data for part 3. 10 percent of train set is validation set \n# and 5 percent of train set is test set.\n# also when loading the dataset (better seen in Dataset Loader), specific classes and affordance labels \n# have been filtered as per the req in part 3. \ndataset = FullShapeDataset(\"/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl\", device=\"cuda\")\ntrain_data, val_data, test_data = create_dataset_splits(dataset, val_ratio=0.1, test_ratio=0.05)\n\n# Ensure test set is not empty\nif len(test_data) == 0:\n    raise ValueError(\"Test dataset is empty. Check your dataset and split ratios.\")\n\n# Select a single object from the test set\ndata_index = 0  # You can adjust this to test different objects\ndata_entry = test_data[data_index]\n\n# Setup CLIP and Renderer\nclip_model, preprocess, resolution = get_clip_model()\nrenderer = MultiViewPointCloudRenderer(image_size=512, base_dist=30, base_elev=10, device=\"cuda\")\n\n# Setup Neural Highlighter\nnet = NeuralHighlighter(depth=5, width=256, out_dim=2, input_dim=3).to(\"cuda\")\n\n# Run the main function for a single object\nmain(\n    data_entry=data_entry,\n    net=net,\n    clip_model=clip_model,\n    renderer=renderer,\n    device=\"cuda\",\n    num_iterations=500,\n    learning_rate=1e-4,\n    output_dir=\"./results\",\n    visualize=True\n)\n\n# Evaluate affordances for the object\nresults = evaluate_single_object(data_entry, net, clip_model, device=\"cuda\")\nprint(\"Evaluation Results:\", results)\n\n# Visualize predictions\nvisualize_single_object(data_entry, net, clip_model, device=\"cuda\", out_dir=\"./results\")\n",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-31T00:38:23.755400Z",
     "iopub.status.busy": "2024-12-31T00:38:23.755011Z",
     "iopub.status.idle": "2024-12-31T00:42:02.604594Z",
     "shell.execute_reply": "2024-12-31T00:42:02.603903Z",
     "shell.execute_reply.started": "2024-12-31T00:38:23.755378Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### New strategy for evaluation\nHyperparam + Strategy Tuning, Then Test Evaluation\nTeammates focus on the below code block for the reporting part of part 3.",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.prompt_strategies import generate_affordance_prompt\n",
    "\n",
    "def train_and_evaluate_shape(\n",
    "    shape_entry, # single shape (object) is passed, a dictionary containing point cloud coordinates, class, affordance\n",
    "    clip_model, # pre trained clip model\n",
    "    strategy, # prompt strategy, e.g basic, affordance class specific\n",
    "    threshold, # binary classification threshold for highlighting. As we know highlighting (or the contrast of highlighting\n",
    "    # is done on the basis of probability distribution generated. so closer the value to 1 the brighter it is. so threshold filters out noise which helps miou)\n",
    "    device=\"cuda\",\n",
    "    num_iterations=200 # Number of optimization steps.\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a neural highlighter for a single 3D shape.\n",
    "    Process:\n",
    "    1. Creates fresh neural network\n",
    "    2. Generates prompt using shape's first affordance and selected strategy\n",
    "    3. Trains network for specified iterations\n",
    "    4. Evaluates IoU across ALL affordances using given threshold\n",
    "    5. Returns average IoU for this shape\n",
    "    \"\"\"\n",
    "    # Extracts shape data from dictionary\n",
    "    shape_coords = shape_entry[\"coords\"] # point cloud coordinates\n",
    "    shape_class = shape_entry[\"shape_class\"] # object class (e.g \"DOOR\")\n",
    "    affs = shape_entry[\"affordances\"] # List of affordances (e.g., [\"Openable\", \"Pushable\"])\n",
    "    label_dict = shape_entry[\"labels_dict\"] # Ground truth labels for each affordance\n",
    "\n",
    "    # Make sure coords is on GPU\n",
    "    if not isinstance(shape_coords, torch.Tensor):\n",
    "        shape_coords = torch.tensor(shape_coords, device=device)\n",
    "\n",
    "\n",
    "\n",
    "    # We initialize fresh neural network for this shape\n",
    "    # change in order to observe better results.\n",
    "    net = NeuralHighlighter(depth=5, width=256, out_dim=2, input_dim=3).to(device)\n",
    "\n",
    "    # Here we fetch the first affordance in the class \n",
    "    main_aff = affs[0]  # e.g \"open\" for door\n",
    "\n",
    "    # Here we generate the affordance and class specific prompt! \n",
    "    prompt_str = generate_affordance_prompt(shape_class, main_aff, strategy)\n",
    "    # here the prompt is converted to text features.\n",
    "    text_feats = encode_text(clip_model, prompt_str, device=device)\n",
    "\n",
    "    # Point cloud renderer is initialized here\n",
    "    # These parameters can also be changed in order to achieve the configuration!\n",
    "    renderer = MultiViewPointCloudRenderer(\n",
    "        image_size=256, base_dist=20, base_elev=10, device=device\n",
    "    )\n",
    "\n",
    "    # Train network through optimization\n",
    "    # Clip is used here to align rendered views with text prompt\n",
    "    net = optimize_point_cloud(\n",
    "        points=shape_coords,\n",
    "        clip_model=clip_model,\n",
    "        renderer=renderer,\n",
    "        encoded_text=text_feats,\n",
    "        log_dir=\"./val_tmp\",\n",
    "        num_iterations=num_iterations,\n",
    "        device=device,\n",
    "        n_views=2 # this can be changed to test different config.\n",
    "    )\n",
    "\n",
    "    # Evaluate IoU across all affordances\n",
    "    with torch.no_grad():\n",
    "        pred_class = net(shape_coords)  # shape [N,2]\n",
    "        highlight_scores = pred_class[:,0] # get highlighting probability\n",
    "\n",
    "    # here we calculate average iou (mIoU) across all affordances of a shape. so one door with three diff affordances.\n",
    "    shape_sum = 0.0\n",
    "    c = 0\n",
    "    for aff in affs:\n",
    "        # convert ground truth to binary for comparison\n",
    "        gt_bin = (label_dict[aff]>0.0).long()\n",
    "        # here we convert predictions to binary using threshold \n",
    "        bin_pred = (highlight_scores >= threshold).long()\n",
    "        # compute IoU for this affordance\n",
    "        iou_val = compute_mIoU(bin_pred, gt_bin)\n",
    "        shape_sum += iou_val\n",
    "        c += 1\n",
    "        \n",
    "    # Here we return the mean iou accross all affordances\n",
    "    shape_mean = shape_sum/c if c>0 else 0.0\n",
    "    return shape_mean\n",
    "\n",
    "\n",
    "def grid_search_validation(\n",
    "    val_dataset, # validation set\n",
    "    clip_model, # clip model\n",
    "    device='cuda', # computation device\n",
    "    strategies=('basic','affordance_specific'), # strategies\n",
    "    thresholds=(0.3, 0.5, 0.7), # classification threshold to try\n",
    "    num_val_objects=3, # number of validation objects to use\n",
    "    num_iterations=200 # the number of iterations\n",
    "):\n",
    "    \"\"\"\n",
    "    For each (strategy, threshold), pick up to 'num_val_objects' shapes from val_dataset,\n",
    "    train & evaluate each shape => average IoU => pick best combo.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # We'll pick 'num_val_objects' shapes from the val dataset for speed\n",
    "    val_indices = list(range(min(num_val_objects, len(val_dataset))))\n",
    "    best_strategy = None\n",
    "    best_threshold = None\n",
    "    best_iou = -1.0\n",
    "    all_results = [] # store all combination results\n",
    "\n",
    "    print(\"[grid_search_validation] Starting shape-by-shape training on validation...\")\n",
    "# grid search loop, each strategy with different thresholds\n",
    "    for strategy in strategies:\n",
    "        for th in thresholds:\n",
    "            print(f\"  Trying strategy='{strategy}' threshold={th}\")\n",
    "            sum_iou = 0.0\n",
    "            count = 0\n",
    "            for idx in val_indices:\n",
    "                shape_entry = val_dataset[idx]\n",
    "                try:\n",
    "                    # here we train and evaluate each shape\n",
    "                    shape_mean = train_and_evaluate_shape(\n",
    "                        shape_entry, \n",
    "                        clip_model,\n",
    "                        strategy,\n",
    "                        th,\n",
    "                        device=device,\n",
    "                        num_iterations=num_iterations\n",
    "                    )\n",
    "                    sum_iou += shape_mean # here we have the mIou (accross all target affordances for each shape)\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Skipped shape idx={idx} due to error: {e}\")\n",
    "                    continue\n",
    "            # here we calculate the mIoU for the current combination! So for example basic strategy with threshold 20 \n",
    "            avg_iou = sum_iou / count if count>0 else 0.0\n",
    "            all_results.append((strategy, th, avg_iou))\n",
    "            print(f\"    => Mean IoU={avg_iou:.3f} over {count} shapes\")\n",
    "\n",
    "            #here we update the parameters to track the best\n",
    "            if avg_iou>best_iou:\n",
    "                best_iou = avg_iou\n",
    "                best_strategy = strategy\n",
    "                best_threshold = th\n",
    "\n",
    "    print(\"\\n[grid_search_validation] Validation combos sorted by best IoU:\")\n",
    "    # finally here we sort and print the best results\n",
    "    sorted_res = sorted(all_results, key=lambda x: x[2], reverse=True)\n",
    "    for sres in sorted_res:\n",
    "        print(f\"    Strategy={sres[0]}, Th={sres[1]}, IoU={sres[2]:.3f}\")\n",
    "\n",
    "    print(f\"\\n[grid_search_validation] Best strategy={best_strategy}, threshold={best_threshold}, average mIoU={best_iou:.3f}\")\n",
    "    return best_strategy, best_threshold, best_iou\n",
    "\n",
    "\n",
    "\n",
    "def test_phase_evaluation(\n",
    "    test_dataset, # test dataset\n",
    "    clip_model, # clip model\n",
    "    best_strategy, # best strategy we got from the validation dataset\n",
    "    best_threshold, # best threshold for best miOu\n",
    "    device='cuda',\n",
    "    num_test_shapes=3, # number of tests to evauluate \n",
    "    num_iterations=200 # training per iteration\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the pipeline on 'num_test_shapes' from test_dataset \n",
    "    using the best strategy & threshold from validation.\n",
    "    Returns the final average test IoU.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # here we select the subset of test shape! \n",
    "    test_indices = list(range(min(num_test_shapes, len(test_dataset))))\n",
    "    sum_test_iou = 0.0\n",
    "    c = 0\n",
    "\n",
    "    print(f\"[test_phase_evaluation] Using strategy={best_strategy}, threshold={best_threshold}\")\n",
    "\n",
    "    # here we evaluate each test shape\n",
    "    for idx in test_indices:\n",
    "        shape_entry = test_dataset[idx]\n",
    "        try:\n",
    "            # train and evaluate single shape using best parameters\n",
    "            shape_mean = train_and_evaluate_shape(\n",
    "                shape_entry,\n",
    "                clip_model,\n",
    "                best_strategy,\n",
    "                best_threshold,\n",
    "                device=device,\n",
    "                num_iterations=num_iterations\n",
    "            )\n",
    "            sum_test_iou += shape_mean\n",
    "            c+=1\n",
    "            print(f\"  -> shape idx={idx}, shape mean IoU={shape_mean:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [Warning] Skipped shape idx={idx} due to error: {e}\")\n",
    "            continue\n",
    "    # finally we calulate and return the final mean test IOU\n",
    "    final_test_iou = sum_test_iou/c if c>0 else 0.0\n",
    "    print(f\"[test_phase_evaluation] Final (average iou) mIoU = {final_test_iou:.3f} (over {c} shapes)\")\n",
    "    return final_test_iou\n",
    "\n",
    "\n",
    "def compute_mIoU(pred_labels, gt_labels):\n",
    "    intersection = ((pred_labels == 1) & (gt_labels == 1)).sum().float()\n",
    "    union = ((pred_labels == 1) | (gt_labels == 1)).sum().float()\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return (intersection / union).item()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Here we pass different strategies and threshold values."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Imports from your code\n",
    "from src.data_loader_fullshape import create_dataset_splits, FullShapeDataset\n",
    "from src.prompt_strategies import generate_affordance_prompt\n",
    "from src.Clip.clip_model import get_clip_model\n",
    "from src.render.cloud_point_renderer import MultiViewPointCloudRenderer\n",
    "from src.neural_highlighter import NeuralHighlighter\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "dataset = FullShapeDataset(\"/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl\", device=device)\n",
    "train_data, val_data, test_data = create_dataset_splits(dataset, val_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "\n",
    "print(f\"[Main] val_data size={len(val_data)}, test_data size={len(test_data)}\")\n",
    "\n",
    "# 3) Load CLIP model\n",
    "clip_model, _, _ = get_clip_model(\"ViT-L/14\")\n",
    "\n",
    "# 4) Validation Phase: search best (strategy, threshold)\n",
    "strategies_list = [\"basic\", \"affordance_specific\"]\n",
    "thresholds_list = [0.3, 0.5]  # let's have 2 thresholds now\n",
    "num_val_objects = 3          # how many shapes we pick from val\n",
    "num_iterations_val = 200     # how many training steps\n",
    "\n",
    "best_strat, best_th, best_val_iou = grid_search_validation(\n",
    "    val_dataset=val_data,\n",
    "    clip_model=clip_model,\n",
    "    device=device,\n",
    "    strategies=strategies_list,\n",
    "    thresholds=thresholds_list,\n",
    "    num_val_objects=num_val_objects,\n",
    "    num_iterations=num_iterations_val\n",
    ")\n",
    "\n",
    "print(f\"\\n[Main] Validation done => best strategy={best_strat}, threshold={best_th}, valIoU={best_val_iou:.3f}\")\n",
    "\n",
    "# 5) Test Phase\n",
    "num_test_shapes = 3\n",
    "test_iterations = 200\n",
    "final_test_iou = test_phase_evaluation(\n",
    "    test_dataset=test_data,\n",
    "    clip_model=clip_model,\n",
    "    best_strategy=best_strat,\n",
    "    best_threshold=best_th,\n",
    "    device=device,\n",
    "    num_test_shapes=num_test_shapes,\n",
    "    num_iterations=test_iterations\n",
    ")\n",
    "print(f\"[Main] Final test IoU => {final_test_iou:.3f}\")\n",
    "\n",
    "# 6) (Optional) Visualize multi-view for the last test shape\n",
    "if len(test_data)>0:\n",
    "    last_idx = min(num_test_shapes, len(test_data)) - 1\n",
    "    last_shape = test_data[last_idx]\n",
    "    print(\"\\n[Main] Visualizing multi-view for the last test shape...\")\n",
    "\n",
    "    # We'll do a quick training again\n",
    "    shape_mean = train_and_evaluate_shape(\n",
    "        last_shape,\n",
    "        clip_model,\n",
    "        best_strat,\n",
    "        best_th,\n",
    "        device=device,\n",
    "        num_iterations=test_iterations\n",
    "    )\n",
    "    # Now we do a manual multi-view color-labeled scatter\n",
    "    coords_test = last_shape[\"coords\"]\n",
    "    if not isinstance(coords_test, torch.Tensor):\n",
    "        coords_test = torch.tensor(coords_test, device=device)\n",
    "\n",
    "    # We can re-run the net\n",
    "    net = NeuralHighlighter(depth=5, width=256, out_dim=2, input_dim=3).to(device)\n",
    "    from src.Clip.clip_model import encode_text\n",
    "    aff_main = last_shape[\"affordances\"][0]\n",
    "    prompt_main = generate_affordance_prompt(last_shape[\"shape_class\"], aff_main, best_strat)\n",
    "    txt_main = encode_text(clip_model, prompt_main, device=device)\n",
    "    renderer = MultiViewPointCloudRenderer(image_size=256, base_dist=20, base_elev=10, device=device)\n",
    "    net = optimize_point_cloud(\n",
    "        points=coords_test,\n",
    "        clip_model=clip_model,\n",
    "        renderer=renderer,\n",
    "        encoded_text=txt_main,\n",
    "        log_dir=\"./test_vis\",\n",
    "        num_iterations=test_iterations,\n",
    "        device=device,\n",
    "        n_views=2\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_ = net(coords_test)[:,0]\n",
    "\n",
    "    # color\n",
    "    color_tensor = pred_.unsqueeze(1)*torch.tensor([204/255,1.0,0.0], device=device) + (1.0 - pred_.unsqueeze(1))*torch.tensor([180/255,180/255,180/255], device=device)\n",
    "    point_cloud = renderer.create_point_cloud(coords_test, color_tensor)\n",
    "    rendered_dict = renderer.render_all_views(point_cloud, n_views=4)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(1, len(rendered_dict), figsize=(4*len(rendered_dict), 4))\n",
    "    for ax, (vname, imgT) in zip(axes, rendered_dict.items()):\n",
    "        ax.imshow(imgT.cpu().numpy())\n",
    "        ax.set_title(vname)\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f\"Test Shape {last_shape['shape_id']} - final threshold={best_th}\")\n",
    "    plt.show()\n"
   ]
  }
 ]
}
